% 互联网


# 技术

## 微服务

### 微服务 (Microservices) 是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块 (Small Building Blocks) 为基础，利用模组化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关 (Language-Independent/Language agnostic) 的 API 集相互通讯。 其核心想法是让服务是由类似 Unix 管道的存取方式使用，而且复杂的服务背后是使用简单 URI 来开放界面，任何服务，任何细粒都能被开放 (exposed)。 微服务是由以单一应用程序构成的小服务，自己拥有自己的行程与轻量化处理，服务依业务功能设计，以全自动的方式部署，与其他服务使用 HTTP API 通讯。同时服务会使用最小的规模的集中管理 (例如 Docker) 能力，服务可以用不同的编程语言与数据库等元件实作[1]。

#### 微服务是一种以业务功能为主的服务设计概念，每一个服务都具有自主运行的业务功能，对外开放不受语言限制的 API (最常用的是 HTTP)，应用程序则是由一个或多个微服务组成。 微服务的另一个对比是单体式应用程序。单体式应用表示一个应用程序内包含了所有需要的业务功能，并且使用像主从式架构 (Client/Server) 或是多层次架构 (N-tier) 实作，虽然它也是能以分散式应用程序来实作，但是在单体式应用内，每一个业务功能是不可分割的。若要对单体式应用进行扩展则必须将整个应用程序都放到新的运算资源（如：虚拟机器） 内，但事实上应用程序中最吃资源、需要运算资源的仅有某个业务部分（例如跑分析报表或是数学算法分析），但因为单体式应用无法分割该部分，因此无形中会有大量的资源浪费的现象。 微服务运用了以业务功能的设计概念，应用程序在设计时就能先以业务功能或流程设计先行分割，将各个业务功能都独立实作成一个能自主执行的个体服务，然后再利用相同的协定将所有应用程序需要的服务都组合起来，形成一个应用程序。若需要针对特定业务功能进行扩充时，只要对该业务功能的服务进行扩展就好，不需要整个应用程序都扩展，同时，由于微服务是以业务功能导向的实作，因此不会受到应用程序的干扰，微服务的管理员可以视运算资源的需要来配置微服务到不同的运算资源内，或是布建新的运算资源并将它配置进去。 虽然使用一般的服务器虚拟化技术就能应用于微服务的管理，但容器技术 (Container Technology) 如 Docker 会更加地适合发展微服务的运算资源管理技术。

### 好处

#### 技术异构性



#### 弹性

#### 扩展

#### 简化部署

##### 各服务的部署独立，这样就部署特定服务更快，有问题也只影响一个服务，并且回滚快，用户更快用到最新功能

#### 与组织结构相匹配

##### 更好的将构架与组织结构相匹配，避免过大的程序库，获得理想的团队大小和生产力

#### 可组织性

#### 可替代性的优化

##### 可以轻易的重写服务，当小零件换掉

### 集成技术

#### 理想型

##### 避免破坏性修改

##### 保证API的技术无关性

##### 易于消费方使用

##### 隐藏内部实现细节

#### 共享数据库

#### 同步与异步

##### req/rsp

###### 同步

###### 异步执行，同步回调

##### event-base

#### 编排与协同





#### 针对req/rsp方式，可以考虑两种技术: RPC和REST

##### RPC

###### 生成客户端桩，二进制性能高

##### REST

#### 向浏览器发数据

##### websocket

#### 基于事件的异步协作

##### 微服务发布事件机制和消费者接受事件机制

##### 生产者向代理(消息队列)发布事件，代理可以向消费者提供订阅服务，并且在事件发生时通知消费者

##### 异步构架的复杂性

#### 服务即是状态机

### 原则

#### 围绕业务概念建模

#### 接受自动化文化

#### 隐藏内部实现细节

#### 让一切都去中心化

#### 可独立部署

#### 隔离失败

#### 高度可观察

### 设计

#### 数据库

##### 数据库 微服务理念中有数个数据库的规划方式。 每个服务都各有一个数据库，同属性的服务可共享同个数据库。 所有服务都共享同个数据库，但是不同表格，并且不会跨域存取。 每个服务都有自己的数据库，就算是同属性的服务也是，数据库并不会共享。 数据库并不会只存放该服务的资料，而是“该服务所会用到的所有资料”。更深层一点的举例：假设有个文章服务，而这个服务可能会需要判断使用者的账号⋯⋯等。那么文章服务的数据库就可以放入使用者的部分资料。此举是为了避免服务之间的相依性，避免文章服务呼叫使用者服务。 数据库的可弃性 实践微服务有许多的做法，但其中一种做法是将数据库作为短期的储存空间而不是储存长期的资料。这意味着数据库可以在离线时被清空。因为它们可以在上线时从事件存储中心恢复，因此也能以内存快取（如：Redis） 作为数据库服务器。但这种做法需要将每个请求当作事件来进行广播。如此一来就可以从事件存储中心重播所有的事件来找回所有的资料。

#### 沟通与事件广播

##### 沟通与事件广播 NSQ 是一个讯息伫列系统、平台。在微服务中所扮演的角色是将讯息、资料传递到其他服务。 此举是异步执行，所以不需要等到其他服务接收到讯息就能够执行下一步。这种方式能够避免服务之间有所牵连、呼叫。 微服务中最重要的就是每个服务的独立与自主，因此服务与服务之间也不应该有所沟通。倘若真有沟通，也应采用异步沟通的方式来避免紧密的相依性问题。要达到此目的，则可用下列两种方式： 事件存储中心（Event Store） 这可以让你在服务丛集中广播事件，并且在每个服务中监听这些事件并作处理，这令服务之间没有紧密的相依性，而这些发生的事件都会被保存在事件存储中心里。这意味着当微服务重新上线、部署时可以重播（Replay）所有的事件。这也造就了微服务的数据库随时都可以被删除、摧毁，且不需要从其他服务中取得资料。 讯息伫列（Message Queue） 这令你能够在服务丛集中广播消息，并传递到每个服务中。具有这个功能的像是 NSQ 或是 RabbitMQ。你能够在 A 服务上广播一个“建立新使用者”的事件，这个事件可以顺便带有新使用者的资料。而 B 服务可以监听这个事件并在接收到之后有所处理。这些过程都是异步处理的，这意味着 A 服务并不需要等到 B 服务处理完该事件后才能继续，而这也代表 A 服务无法取得 B 服务的处理结果。与事件存储中心近乎相似，但有所不同的是：讯息伫列并不会保存事件。一旦事件被消化（接收）后就会从伫列中消失，这很适合用在像发送欢迎信件的时机。

#### 服务探索

##### 服务探索 单个微服务在上线的时候，会向服务探索中心（如：Consul）注册自己的 IP 位置、服务内容，如此一来就不需要向每个微服务表明自己的 IP 位置，也就不用替每个微服务单独设定。当服务需要呼叫另一个服务的时候，会去询问服务探索中心该服务的 IP 位置为何，得到位置后即可直接向目标服务呼叫。 这么做的用意是可以统一集中所有服务的位置，就不会分散于每个微服务中，且服务探索中心可以每隔一段时间就向微服务进行健康检查（如透过：TCP 呼叫、HTTP 呼叫、Ping），倘若该服务在时间内没有回应，则将其从服务中心移除，避免其他微服务对一个无回应的服务进行呼叫。

### 特点

#### 一个微服务架构的应用程序有下列特性： 每个服务都容易被取代。 服务是以能力来组织的，例如使用者界面、前端、推荐系统、账单或是物流等。 由于功能被拆成多个服务，因此可以由不同的编程语言、数据库实作。 架构是对称而非分层（即生产者与消费者的关系）。 一个微服务架构： 适用于具持续交付 (Continuous Delivery) 的软件开发流程。 与服务导向架构 (Service-Oriented Architecture) 不同，后者是整合各种业务的应用程序，但微服务只属于一个应用程序。

### 误解

#### 微服务这个名词令许多人以为是非常轻量、非常微小的，且以为透过该理念实作程式就能够达到下列效果： 微服务很轻量。 程式码将会变得更加地简洁。 变得更简单、开发时程变短。 微服务处理的事情变得更单一。 但这些是误解，实际上： 由于服务是独立自主的（也称：真空性），除了需要能够有自己的一套执行方式外，还不应该仰赖另一个服务。为此，服务内会有着与其他服务相同的逻辑，这也导致了服务并不轻量。这部分有两派说法，分别是在服务之间建立同套资源库、工具，但这可能导致额外的相依性存在。而另一种说法则是传统地将程式码复制与贴上，这将避免相依性问题，但在全域修改时可能不易管控，需要分散管理。 微服务属于分布式系统的概念之一，程式码并不会因此变得简单、短少，反而有可能为了处理外来的事件而变得更多。 微服务需要额外处理事件的广播、甚至是分布式的错误回溯问题，这导致开发时可能会更加地复杂，且花上更多时间在处理错误上。 基于第一点误解，微服务为了自主有可能会跨域实作，如文章服务有可能会带有使用者服务的理念，所以在处理事情上并不会特别专一。

## 模块化和组件化和服务化

### 模块化的目的是为了重用，模块化后可以方便重复使用和插拨到不同的平台，不同的业务逻辑过程中。

### 组件化的目的是为了解耦，把系统拆分成多个组件，分离组件边界和责任，便于独立升级和维护。

### 服务化是一种粗粒度、松耦合的以服务为中心的架构，服务之间通过定义明确的协议和接口进行通信。

#### 如何管理和协同这些大量的RPC才是最麻烦的事情。所以，一般提到的“服务化”更多指的是对RPC的管理。服务化一般关注服务注册，服务协调，服务可用性，服务通讯协议和内容交换等。

### SOA关注的是服务重用，微服务在关注服务重用的同时，也同时关注快速交付；

## RESTful API

### 设计风格

#### 资源是由URI来指定。 对资源的操作包括获取、创建、修改和删除资源，这些操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法。 通过操作资源的表现形式来操作资源。 资源的表现形式则是XML或者HTML，取决于读者是机器还是人，是消费web服务的客户软件还是web浏览器。当然也可以是任何其他的格式。

### 特点

#### 统一接口(Uniform Interface)

#### Stateless

#### Cacheable

#### Client-Server

#### Layered System

#### Code on Demand (optional)

### 架构约束

#### 客户-服务器（Client-Server） 通信只能由客户端单方面发起，表现为请求-响应的形式。

#### 无状态（Stateless） 通信的会话状态（Session State）应该全部由客户端负责维护。

#### 缓存（Cache） 响应内容可以在通信链的某处被缓存，以改善网络效率。

#### 统一接口（Uniform Interface） 通信链的组件之间通过统一的接口相互通信，以提高交互的可见性。

#### 分层系统（Layered System） 通过限制组件的行为（即每个组件只能“看到”与其交互的紧邻层），将架构分解为若干等级的层。

#### 按需代码（Code-On-Demand，可选） 支持通过下载并执行一些代码（例如Java Applet、Flash或JavaScript），对客户端的功能进行扩展。

### 应用于Web服务

#### 匹配具象状态传输设计风格的Web API称为RESTful API。它从以下三个方面资源进行定义： 直观简短的资源地址：URI，比如：http://example.com/resources/。 传输的资源：Web服务接受与返回的互联网媒体类型，比如：JSON，XML，YAML等。 对资源的操作：Web服务在该资源上所支持的一系列请求方法（比如：POST，GET，PUT或DELETE）。 下表列出了在实现RESTful API时HTTP请求方法的典型用途。



### 优点

#### 可更高效利用缓存来提高响应速度 通讯本身的无状态性可以让不同的服务器的处理一系列请求中的不同请求，提高服务器的扩展性 浏览器即可作为客户端，简化软件需求 相对于其他叠加在HTTP协议之上的机制，REST的软件依赖性更小 不需要额外的资源发现机制 在软件技术演进中的长期的兼容性更好

### REST是完全不同的思路，它充分利用了HTTP协议的4个主要verb把RPC操作分成4类： - GET：进行幂等的资源获取操作 - POST：创建资源 - PATCH：修改资源 - DELETE：删除资源 仔细想一下这其实就是数据库的CRUD操作 POST=create GET=read PATCH=update DELETE=delete

## RPC

### 远程过程调用是一个分布式计算的客户端-服务器（Client/Server）的例子，它简单而又广受欢迎。远程过程调用总是由客户端对服务器发出一个执行若干过程请求，并用客户端提供的参数。执行结果将返回给客户端。由于存在各式各样的变体和细节差异，对应地派生了各式远程过程调用协议，而且它们并不互相兼容

### 为了允许不同的客户端均能访问服务器，许多标准化的 RPC 系统应运而生了。其中大部分采用接口描述语言（Interface Description Language，IDL），方便跨平台的远程过程调用

### JSON-RPC

#### JSON-RPC，是一个无状态且轻量级的远程过程调用（RPC）传送协议，其传递内容通过 JSON 为主。相较于一般的 REST 通过网址（如 GET /user）调用远程服务器，JSON-RPC 直接在内容中定义了欲调用的函数名称（如 {"method": "getUser"}），这也令开发者不会陷于该使用 PUT 或者 PATCH 的问题之中。 本规范主要定义了一些数据结构及其相关的处理规则。它允许运行在基于 Socket、HTTP 等诸多不同消息传输环境的同一进程中。其使用 JSON（RFC 4627）作为数据格式。

#### 请求对象

##### 发送一个请求对象至服务端代表一个 RPC 调用，一个请求对象包含下列成员： jsonrpc method params id 服务端必须回答相同的值如果包含在响应对象。这个成员用来两个对象之间的关联上下文。在请求对象中不建议使用 NULL 作为 id 值，因为该规范将使用空值认定为未知id的请求。另外，由于JSON-RPC 1.0 的通知使用了空值，这可能引起处理上的混淆。使用小数是不确定性的，因为许多十进制小数不能精准的表达为二进制小数。 通知 没有包含 id 成员的请求对象为通知， 作为通知的请求对象表明客户端对相应的响应对象并不感兴趣，本身也没有响应对象需要返回给客户端。服务端必须不回复一个通知，包含那些批量请求中的。 由于通知没有返回的响应对象，所以通知不确定是否被定义。同样，客户端不会意识到任何错误（例如参数缺省，内部错误）。 参数结构 RPC 调用如果存在参数则必须为基本类型或结构化类型的参数值，要么为索引数组，要么为关联数组对象。 索引：参数必须为数组，并包含与服务端预期顺序一致的参数值。 关联名称：参数必须为对象，并包含与服务端相匹配的参数成员名称。没有在预期中的成员名称可能会引起错误。名称必须完全匹配，包括方法的预期参数名以及大小写。

#### 响应对象

##### 当发起一个 RPC 调用时，除通知之外，服务端都必须回复响应。响应表示为一个 JSON 对象，使用以下字段： jsonrpc result error id 响应对象必须包含 result 或 error 字段，但两个字段不能同时存在。 错误对象 当一个 RPC 调用遇到错误时，返回的响应对象必须包含错误成员参数，并且为带有下列成员参数的对象： code message data -32768 至 -32000 为保留的预定义错误代码。在该范围内的错误代码不能被开发者自己定义，保留下列以供将来使用。错误代码基本与 XML-RPC 建议的一样

#### 批量调用

##### 批量调用 当需要同时发送多个请求对象时，客户端可以发送一个包含所有请求对象的数组。 当批量调用的所有请求对象处理完成时，服务端则需要返回一个包含相对应的响应对象数组。每个响应对象都应对应每个请求对象，除非是通知的请求对象。服务端可以并发的，以任意顺序和任意宽度的并行性来处理这些批量调用。 这些相应的响应对象可以任意顺序的包含在返回的数组中，而客户端应该是基于各个响应对象中的 id 成员来匹配对应的请求对象。 若批量调用的 RPC 操作本身非一个有效 JSON 或一个至少包含一个值的数组，则服务端返回的将单单是一个响应对象而非数组。若批量调用没有需要返回的响应对象，则服务端不需要返回任何结果且必须不能返回一个空数组给客户端。

#### JSON-RPC无法像REST一样享受HTTP的各种优点(standard interface, stateless, cache..)，又必须承担HTTP作为基于文本的协议，payload过大传输的成本以及序列化反序列化的开销

### gRPC

#### A simple RPC where the client sends a request to the server using the stub and waits for a response to come back, just like a normal function call

#### A server-side streaming RPC where the client sends a request to the server and gets a stream to read a sequence of messages back.

#### A client-side streaming RPC where the client writes a sequence of messages and sends them to the server, again using a provided stream.

#### A bidirectional streaming RPC where both sides send a sequence of messages using a read-write stream. The two streams operate independently, so clients and servers can read and write in whatever order they like: for example, the server could wait to receive all the client messages before writing its responses, or it could alternately read a message then write a message, or some other combination of reads and writes. The order of messages in each stream is preserved

#### protoc -I ../../protos --grpc_out=. --plugin=protoc-gen-grpc=`which grpc_cpp_plugin` ../../protos/route_guide.proto protoc -I ../../protos --cpp_out=. ../../protos/route_guide.proto

### 面向终端用户的尽量用Restful HTTP, 面向内部系统尽量用二进制的RPC, 性能更高

### 数据序列化格式

#### Compare various data serialization libraries for C++. Thrift Protobuf Boost.Serialization Msgpack Cereal Avro Capnproto Flatbuffers YAS





## coroutine

### 总结一下，实现Coroutine主要有三种方法： 改虚拟机 好处：实现简单，跟语言的其他功能是正交的 坏处：只要你的改动不能merge回主分支，你就会一辈子蛋疼地conflict下去 语言直接提供continuation 好处：有continuation可以实现非常强大的控制流语句，Coroutine也只是其中的一个作用而已，你不需要专门为Coroutine做什么 坏处：这样的语言并不常见 要求SHIT!只能出现在SHIT_CALLABLE!函数里面，并且调用SHIT_CALLABLE!函数要用特殊的语法SHIT_CALL!，然后解开成一个大switch 好处：continuation毕竟是闭包，各种闭包群P容易给GC造成压力（这是Don Syme告诉我的，当初我发邮件问他为什么F#的computation expression的循环不支持break，他说这样就不能编译成状态机了），而直接改状态机并没有这个问题，甚至是像C++这样没有GC只有shared_ptr的语言也可以完美支持 坏处：需要改语法

## c++11 concurrency

### std::async

#### Not always it starts a new thread; pass std::launch::async as a first parameter to force it. the std::~future destructor can block until new thread finishes Normally we expect that only .get() or .wait() blocks, but for std::future returned from std::async destructor also may block, so be careful not to block your main thread just by forgetting about it. if the std::future is stored in a temporary-life object, std::async call will block on spot, so the following block will take 10 seconds if you remove the auto

### std::packaged_task

#### std::packaged_task by itself has nothing to do with threads: it is just a functor and a related future. std::packaged_task is not copyable, so you move it to new task with std::move.

### std::promise

#### std::promise is a powerful mechanism, for example you can pass a value to new thread without need of any additional synchronizing mechanism

### use std::async only for simple things, e.g. to make some call non-blocking, but bear in mind the comments on blocking above. use std::packaged_task to easily get future, and run it as a separate thread use std::promise when you need more control over the future

### 工作线程数是不是设置的越大越好

#### 一来服务器CPU核数有限，同时并发的线程数是有限的，1核CPU设置10000个工作线程没有意义

#### 线程切换是有开销的，如果线程切换过于频繁，反而会使性能降低

### 调用sleep()函数的时候，线程是否一直占用CPU

#### 不占用，等待时会把CPU让出来，给其他需要CPU资源的线程使用

### 即使是单核，使用多线程也是有意义的

#### 多线程编码可以让我们的服务/代码更加清晰，有些IO线程收发包，有些Worker线程进行任务处理，有些Timeout线程进行超时检测

#### 如果有一个任务一直占用CPU资源在进行计算，那么此时增加线程并不能增加并发

#### 通常来说，Worker线程一般不会一直占用CPU进行计算，此时即使CPU是单核，增加Worker线程也能够提高并发，因为这个线程在休息的时候，其他的线程可以继续工作

### IO线程与工作线程通过队列解耦类模型

#### 这个线程模型应用很广，符合大部分场景，这个线程模型的特点是，工作线程内部是同步阻塞执行任务的。因此可以通过增加Worker线程数来增加并发能力

#### 1）有少数几个IO线程监听上游发过来的请求，并进行收发包（生产者） 2）有一个或者多个任务队列，作为IO线程与Worker线程异步解耦的数据传输通道（临界资源） 3）有多个工作线程执行正真的任务（消费者）

### 纯异步线程模型

#### 任何地方都没有阻塞，这种线程模型只需要设置很少的线程数就能够做到很高的吞吐量，Lighttpd有一种单进程单线程模式，并发处理能力很强，就是使用的的这种模型。

#### 缺点

##### 1）如果使用单线程模式，难以利用多CPU多核的优势 2）程序员更习惯写同步代码，callback的方式对代码的可读性有冲击，对程序员的要求也更高 3）框架更复杂，往往需要server端收发组件，server端队列，client端收发组件，client端队列，上下文管理组件，有限状态机组件，超时管理组件的支持

### 线程数计算

#### 通过上面的分析，Worker线程在执行的过程中，有一部计算时间需要占用CPU，另一部分等待时间不需要占用CPU，通过量化分析，例如打日志进行统计，可以统计出整个Worker线程执行过程中这两部分时间的比例，例如： 1）时间轴1，3，5，7【上图中粉色时间轴】的计算执行时间是100ms 2）时间轴2，4，6【上图中橙色时间轴】的等待时间也是100ms 得到的结果是，这个线程计算和等待的时间是1：1，即有50%的时间在计算（占用CPU），50%的时间在等待（不占用CPU）： 1）假设此时是单核，则设置为2个工作线程就可以把CPU充分利用起来，让CPU跑到100% 2）假设此时是N核，则设置为2N个工作现场就可以把CPU充分利用起来，让CPU跑到N*100%

#### 一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的

#### N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。

#### 最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 最佳线程数目 = （线程等待时间与线程CPU时间之比 + 1）* CPU数目 线程等待时间所占比例越高，需要越多线程。线程CPU时间所占比例越高，需要越少线程

### 是否使用线程池就一定比使用单线程高效呢？ 答案是否定的，比如Redis就是单线程的，但它却非常高效，基本操作都能达到十万量级/s。从线程这个角度来看，部分原因在于： 1. 多线程带来线程上下文切换开销，单线程就没有这种开销 2. 锁 Redis很快 更本质的原因在于：Redis基本都是内存操作，这种情况下单线程可以很高效地利用CPU。而多线程适用场景一般是：存在相当比例的IO和网络操作。

## IO多路复用(reactor & proactor)

### 一般地,I/O多路复用机制都依赖于一个事件多路分离器(Event Demultiplexer)。分离器对象可将来自事件源的I/O事件分离出来，并分发到对应的read/write事件处理器(Event Handler)。开发人员预先注册需要处理的事件及其事件处理器（或回调函数）；事件分离器负责将请求事件传递给事件处理器

### Reactor模式采用同步IO，而Proactor采用异步IO。

### 在Reactor中，事件分离器负责等待文件描述符或socket为读写操作准备就绪，然后将就绪事件传递给对应的处理器，最后由处理器负责完成实际的读写工作。

### 在Proactor模式中，处理器--或者兼任处理器的事件分离器，只负责发起异步读写操作。IO操作本身由操作系统来完成。传递给操作系统的参数需要包括用户定义的数据缓冲区地址和数据大小，操作系统才能从中得到写出操作所需数据，或写入从socket读到的数据。事件分离器捕获IO操作完成事件，然后将事件传递给对应处理器

### 在Reactor中实现读：

#### - 注册读就绪事件和相应的事件处理器 - 事件分离器等待事件 - 事件到来，激活分离器，分离器调用事件对应的处理器。 - 事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。

### 在Proactor中实现读：

#### - 处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，处理器无视IO就绪事件，它关注的是完成事件。 - 事件分离器等待操作完成事件 - 在分离器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分离器读操作完成。 - 事件分离器呼唤处理器。 - 事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分离器。

### reactor：能收了你跟俺说一声。 proactor: 你给我收十个字节，收好了跟俺说一声。

## threads vs events

### in "thread driven" runtimes, when a request comes in, a new thread is created and all the handling is done in that thread.

### in "event driven" runtimes, when a request comes in, the event is dispatched and handler will pick it up. When? In Node.js, there is an "event loop" which basically loops over all the pieces of code that need to be executed and executes them one by one. So the handler will handle the event once event loop invokes it. The important thing here is that all the handlers are called in the same thread - the event loop doesn't have a thread pool to use, it only has one thread. Of course there is more that one thread in Node.js process, some of them are related to I/O. But your app logic is handled in one thread

### 线程

#### 上下文切换，由于线程堆栈，大量的线程需要大量的内存。 锁的使用会产生额外的开销

### 事件

#### 通过不使用阻塞/同步I / O，多个I / O操作重叠，I / O并行性不需要CPU同时进行。可以用户态调度

#### 事件循环和注册事件处理程序的概念产生控制反转。 代码不是按顺序操作，而是组织为一组碎片化的事件处理程序和回调。

#### 开发者有责任处理和恢复事件处理程序之间的状态

#### 虽然单线程事件循环模型主要适用于I / O绑定的应用程序，但缺省情况下很难利用实际的CPU并发性和利用多个内核

### 前者使用少量使用显式消息传递的进程，而后者基于大量使用共享数据的小进程。 因此，面向消息的系统类似于事件驱动的系统，而面向过程的系统则对应于基于线程的系统 1. 两种模型都是双重对立的 。 用一个模型编写的程序可以直接映射到基于其他模型的等效程序。 2. 两种模型在逻辑上都是等价的 ，尽管它们使用了不同的概念并提供了不同的语法。 3. 考虑到使用相同的调度策略，用两种模型编写的程序的性能基本相同 。



### 正交管理概念

#### 任务管理

##### 程序中的执行流程通常被分成共存的独立任务。 管理这些任务的并发执行需要一个关于如何在调度之类的任务之间切换的管理概念

##### 串行任务管理顺序运行任务完成，然后切换到下一个任务。 虽然这种策略可以防止孤立执行导致的状态冲突，但它不允许利用真正的并行性。 另外，等待I / O的长时间运行的任务或任务将延迟执行其他未决任务。

##### 抢先式任务管理改为可同时重叠执行多个任务，并利用多个核心。 但是，任务将在外部进行计划，因此任务不知道任务管理

##### 合作任务管理，保留了两种模型的一些优点。 任务合作并明确地产生，但使代码更容易推理。 单线程协作任务管理有助于处理不变量和状态。 对于多线程代码，协作任务管理通常会减少上下文切换的次数。

#### 堆栈管理

##### 在基于线程的模型中，任务具有自己的堆栈，因此（自动）堆栈管理是一项固有功能。

##### 基于事件的系统需要不同的任务堆栈处理。 由于在这种情况下逻辑任务的执行流由一系列调度事件和相应的事件处理程序执行来表示，因此没有直接的堆栈概念。 而且，不同的过程处理与逻辑任务相对应的事件序列，所以状态处理必须在多个事件处理程序中断开。 因此，堆栈必须由开发人员明确提供(堆栈翻录)。 在产生之前，任务的堆栈必须被序列化并存储。 当事件处理程序稍后继续执行时，它必须首先加载并重建相应任务的堆栈。

##### 闭包是封装其引用环境的函数（即“堆栈”）。 持续是用于封装控制状态的特殊关闭

#### IO管理

##### I / O管理负责I / O操作，并可分为同步和异步管理接口。

#### 冲突管理

##### 串行任务是独占执行的，协作任务提供所有产出操作之间的原子语义。 这使得关于不变量的推理非常容易。 确保不变量对于抢先式任务管理来说更复杂，并且需要同步机

#### 共享分区

##### 共享状态和一致性保持与任务和冲突管理相关。 因此，划分数据并限制访问状态可以减少冲突的可能性。 例如，线程本地状态不必共享，并且可以显式分区

### 传统的基于事件的系统主要基于协作任务管理和需要堆栈翻转的手动堆栈管理，但基于线程的系统通常使用抢先式任务管理和自动堆栈管理。 最终，他们倾向于使用协作式任务管理的模型，但将开发人员从堆栈管理负担中释放出来， 如图4.5所示。 这种模型简化了并发性反射，需要最少的冲突管理并与两种I / O管理模型协调一致

### 最近的一些事件驱动系统，如node.js，已经非常接近这个预期的模型。 他们依赖闭包作为语言原语，将堆栈数据封装到回调函数中，从而减轻堆栈翻录的负担

### 对于大规模连接并发性，使用异步/非阻塞I / O操作的事件驱动服务器体系结构似乎更受欢迎，因为它们在严重并发下提供稍好的可伸缩性。 这些服务器即使在处理数千个并发连接时也需要更少的内存。 而且，它们不需要专门的线程库。 另一方面，成熟的线程实现（如本机POSIX线程库[ Mol03 ]）仍然可以提供合理的性能，即使是高度并发的服务器实现。

## async/await和有栈协程的区别

### 前者（async/await）在函数返回前把那些变量临时保存在堆的某个地方，然后把存放地址传回去，当你想返回现场的时候，把这些变量恢复，并跳转回离开时候那个语句；持有指针语义的c/c++语言则略麻烦：因为可能这些局部变量中有谁持有另一个局部变量的地址，这样“值语义”的恢复就会把他们变成野指针，所以需要在进入函数时所有的局部变量和函数参数都在堆上分配，这样就不会有谁持有离开时栈上下文的指针了，换句话说，对c/c++来说，这是一种无栈协程(有些自己写的无栈协程库提供你在堆上面分配局部变量的接口，或者强迫你在进入这个函数前把要用到的所有局部变量在堆上面分配好内存)，其它语言只要没有值语义或变量天生不放栈上就没这个概念。如果使用闭包语法返回现场，可以只需要恢复闭包中捕获的变量；对于c++，在离开现场时不能提前析构掉那些没有被捕获的变量（否则析构顺序未必是构造顺序的反序，其实这个c++规则真是没必要）。所以从C++的观点来说，这是一种彻头彻尾的“假”函数返回(有垃圾回收器的语言倒是有可能走到async之后的语句后，回收前面已经不用的临时变量)。

### 后者（有栈协程）在离开前只需要把函数调用中可能被破坏的callee-saved 寄存器给保存在当前栈就完事了（别的协程和当前协程栈是完全隔离的，不会破坏自己堆栈），跳转回来的时候把在栈中保存的寄存器都恢复了并跳转回离开时候那个语句就行了。综上：前者（尤其是c、c++）需要编译器的特殊支持，对使用了async/await语义的函数中的局部变量的分配，恢复进行些特殊的处理；后者则只需要写写汇编就搞定了（一般需要给 进入协程入口函数，协程间切换，协程函数入口函数返回后回收协程资源并切换去另一个协程 这3个地方写点汇编，也有的协程库把这3种情况都统一起来处理）。

### 语法友好度

#### 语法友好度：衡量这个玩意儿的标准，莫过于“逻辑聚合性”：逻辑相关的代码能否写在相近的代码处。例如 redis/nginx中处处可见这种上下文被分割的代码，因为任何一个“暂时不能完成“的场景都会把场景前后代码逻辑写在完全不同的两个函数里。。对于async/await 或无栈协程语义，c/c++在没有闭包之前的，还需要达夫设备跳转回离开现场的那行代码，有了闭包之后，上下文之间就只被return ( [xxx](){ 分开了，代码可以认为基本没有被分割( C# 新版js, VC和clang实验性的resumable function连这点分开都没有了)；不过依然远远比不上有栈协程，因为他语法完全是常规的函数调用/函数返回，使用hook之类的手法甚至可以把已有的阻塞代码自动yield加无阻塞化（参见libco, libgo）。

#### 前者在得到现代化编译器辅助后，和后者相近但依然有差距且容易对一些常识产生挑战；后者语法非常适合传统编程逻辑。

### 时间/空间效率

#### 时间/空间效率：async/await 语义执行的是传统的函数调用函数返回流程，没有对栈指针进行手工修改操作，cpu对return stack buffer的跳转预测优化继续有效；有栈协程需要在创建时根据协程代码执行的最坏情况提前分配好协程函数的栈，这往往都分配的过大而缺乏空间效率，而且在协程间切换的时候手工切换栈，从而破坏了return stack buffer跳转预测，协程切换后函数的每一次返回都意味着一次跳转预测失效，所以流程越复杂有栈协程的切换开销越大（非对称调度的有栈协程会降低一些这方面的开销，boost新版有栈协程彻底抛弃了对称协程）。对于async/await 语义实践的无栈协程，如果允许提前析构不被捕获的C++变量，或者你返回前手工销毁或者你用的是带垃圾回收器的语言，空间效率会更佳。

#### 前者远胜后者，而且后者会随着你业务复杂度加深以及cpu流水线的变长（还好奔4过后的架构不怎么涨了）而不断变差。笔者写的yuanzhubi/call_in_stack yuanzhubi/local_hook， 以及一个没有开源的jump assembler（把有栈协程切换后的代码输出的汇编语句中的ret指令全部换成pop+jmp指令再编译，避开return stack buffer 预测失败）都是来优化有栈协程在时间/空间的表现的 。

### 调度

#### 调度：其实2者都是允许用户自己去管理调度事宜的，不过前者必须返回由调度函数选择下一个无栈协程的切入，后者允许”深度优先调度“：即当某个协程发现有“暂时不能完成“的场景时自己可以根据当前场景选择一个逻辑相关的协程进行切入，提升内存访问局部性，不过这对使用者的要求和业务侵入度非常高。

#### 整体而言的话，可以认为在这一项：前者和后者大致持平，前者是集中式管理而后者是分布式管理，后者可以挖掘的潜力更高但对使用者要求很高且未必能适应业务的变更。

### 结论：性能上，前者有一定时间优势但不是精雕细琢的多用途公共开源组件完全可以忽略，而空间上前者超越后者很多；易用度上，前者正在快速演进 慢慢的追上后者（c#这样的async/await鼻祖已经完全不存在这个问题）；和已有组件的可结合度上，后者始终保持优势（不管已有组件是源码还是二进制）

## WebRTC和WebSocket

### WebRTC apps need a service via which they can exchange network and media metadata, a process known as signaling. However, once signaling has taken place, video/audio/data is streamed directly between clients, avoiding the performance cost of streaming via an intermediary server.

### WebSocket on the other hand is designed for bi-directional communication between client and server. It is possible to stream audio and video over WebSocket (see here for example), but the technology and APIs are not inherently designed for efficient, robust streaming in the way that WebRTC is.



## IPFS（InterPlanetary File System，星际文件系统）

### 集一些成功系统（分布式哈希表、BitTorrent、Git、自认证文件系统）的优势于一身

### 一个面向全球的、点对点的分布式版本文件系统，目标是为了补充（甚至是取代）目前统治互联网的超文本传输协议（HTTP），将所有具有相同文件系统的计算设备连接在一起

### IPFS的应用意义

#### 第一，可以为内容创作带来一定的自由。 第二，可以降低存储和带宽成本。 第三，可以与区块链完美结合。 第四，可以为传统应用提供分布式缓存方案。

### 特点

#### 内容可寻址：通过文件内容生成唯一哈希值来标识文件，而不是通过文件保存位置来标识。相同内容的文件在系统中只会存在一份，节约存储空间

#### 版本化：可追溯文件修改历史

#### 点对点超媒体：P2P 保存各种各样类型的数据

### HTTP协议的不足

#### HTTP的效率低下，并且服务器昂贵 历史文件被删除 HTTP的中心化限制了发展机会 网络应用过于依赖主干网

### IPFS的出现，则是为了解决中心化web的这些问题。它从本质上改变了网络数据的分发机制

## rtsp/rtmp直播



# 系统

## 多线程

### 好处

#### (1)因为多线程彼此之间采用相同的地址空间，共享大部分的数据，这样和多进程相比，代价比较节俭，因为多进程的话，启动新的进程必须分配给它独立的地址空间，这样需要数据表来维护代码段，数据段和堆栈段等等。

#### (2)多线程和多进程相比，一个明显的优点就是线程之间的通信了，对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行，这种方式不仅费时，而且很不方便。但是对于多线程就不一样了。他们之间可以直接共享数据，比如最简单的方式就是共享全局变量。但是共享全部变量也要注意哦，呵呵，必须注意同步，不然后果你知道的。呵呵。

#### (3)在多cpu的情况下，不同的线程可以运行不同的cpu下，这样就完全并行了。

### 创建多线程根本没有让程序快多少，也没有提高多少cpu利用率，甚至可能让cpu利用率下降。唯一能够确定的是多线程能够避免界面假死

### 多处理器系统也只有一个待运行的线程队列，内存中也只有一个操作系统拷贝，而且也只有一个内存系统，但是会有多个cpu同时运行不同的线程。

### 所有cpu都要和这个内存系统通信，但是只有一条总线，那么这无疑会造成总线紧张，限制整体的速度了。

### 解决这个问题的办法还是利用cpu的缓存机制，每个cpu都有自己的缓存。如果有2个cpu的缓存存储的是同一内存数据的内容，其中一个cpu的缓存更新了，另外一个cpu的缓存也必须更新，这就是所谓的缓存一致性。编程多线程程序的一个很重要的一点就是避免因为缓存一致性引起的缓存更新风暴。

### 多cpu不能成倍提高速度的原因是任务的某些部分是必须串行处理的

### 如果使用多线程，那么就必须考虑线程同步，而线程同步又是导致速度降低的关键

### 加快多线程程序的吞吐速度。 方法一，把一个任务分解为多个可以子任务。(避免io阻塞) 方法二，缓存多线程的共享数据。 方法三，如果线程数目有限，就不要共享数据。 做法是为每一个线程实例化一个单独的数据，其实就是为每一个线程分配一块数据使用。这样没有线程同步操作了，速度可以尽可能的提示。 方法四，如果没办法确定线程数目到底有多少，那么使用部分共享吧。(使用多个资源池代替一个资源池)

## 进程: 资源分配的最小单元

### 进程（英语：process），是计算机中已运行程序的实体。进程为曾经是分时系统的基本运作单位。在面向进程设计的系统（如早期的UNIX，Linux 2.4及更早的版本）中，进程是程序的基本执行实体；在面向线程设计的系统（如当代多数操作系统、Linux 2.6及更新的版本）中，进程本身不是基本运行单位，而是线程的容器。

### 程序本身只是指令、数据及其组织形式的描述，进程才是程序（那些指令和数据）的真正运行实例。 若干进程有可能与同一个程序相关系，且每个进程皆可以同步（循序）或异步（平行）的方式独立运行。现代计算机系统可在同一段时间内以进程的形式将多个程序加载到存储器中，并借由时间共享（或称时分复用），以在一个处理器上表现出同时（平行性）运行的感觉。 同样的，使用多线程技术（多线程即每一个线程都代表一个进程内的一个独立执行上下文）的操作系统或计算机架构，同样程序的平行线程，可在多CPU主机或网络上真正同时运行（在不同的CPU上）。

### 进程在运行时，状态（state）会改变。所谓状态，就是指进程目前的动作： 新生（new）：进程新产生中。 运行（running）：正在运行。 等待（waiting）：等待某事发生，例如等待用户输入完成。亦称“阻塞”（blocked） 就绪（ready）：排班中，等待CPU。 结束（terminated）：完成运行。

### 开销

#### 1。对于 Windows 系统来说，【开桌子】的开销很大，因此 Windows 鼓励大家在一个桌子上吃菜。因此 Windows 多线程学习重点是要大量面对资源争抢与同步方面的问题。 2。对于 Linux 系统来说，【开桌子】的开销很小，因此 Linux 鼓励大家尽量每个人都开自己的桌子吃菜。这带来新的问题是：坐在两张不同的桌子上，说话不方便。因此，Linux 下的学习重点大家要学习进程间通讯的方法。

#### 核心切换开销

## 线程: 运算调度的最小单位, 是进程中的实际运作单位

### 线程（英语：thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。在Unix System V及SunOS中也被称为轻量进程（lightweight processes），但轻量进程更多指内核线程（kernel thread），而把用户线程（user thread）称为线程。

### 线程是独立调度和分派的基本单位。线程可以操作系统内核调度的内核线程，如Win32线程；由用户进程自行调度的用户线程，如Linux平台的POSIX Thread；或者由内核与用户进程，如Windows 7的线程，进行混合调度。

### 同一进程中的多条线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。但同一进程中的多个线程有各自的调用栈（call stack），自己的寄存器环境（register context），自己的线程本地存储（thread-local storage）。

### 一个进程可以有很多线程，每条线程并行执行不同的任务。

### 在多核或多CPU，或支持Hyper-threading的CPU上使用多线程程序设计的好处是显而易见，即提高了程序的执行吞吐率。在单CPU单核的计算机上，使用多线程技术，也可以把进程中负责IO处理、人机交互而常被阻塞的部分与密集计算的部分分开来执行，编写专门的workhorse线程执行密集计算，从而提高了程序的执行效率。

### 线程有四种基本状态，分别为： 产生（spawn） 中断（block） 非中断（unblock） 结束（finish）

## 协程

### 协程可以通过yield来调用其它协程。通过yield方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的。

### 因为相对于子例程，协程可以有多个入口和出口点，可以用协程来实现任何的子例程。事实上，正如Knuth所说：“子例程是协程的特例。” 每当子例程被调用时，执行从被调用子例程的起始处开始；然而，接下来的每次协程被调用时，从协程返回（或yield）的位置接着执行。

### 程序由两个分支组成。子程序处理完后，用 yield 将自己挂起，并返回主程序。主程序通过 send 唤起子程序并传入数据。如此交替进行。 def printer(): counter = 0 while True: string = (yield) print('[{0}] {1}'.format(counter, string)) counter += 1 if __name__ == '__main__': p = printer() next(p) p.send('Hi') p.send('My name is hsfzxjy.') p.send('Bye!')

## 进程调度



#### 吞吐vs.响应

##### 任何操作系统的调度器设计只追求2个目标：吞吐率大和延迟低。 这2个目标有点类似零和游戏，因为吞吐率要大，势必要把更多的时间放在做真实的有用功，而不是把时间浪费在频繁的进程上下文切换；而延迟要低，势必要求优先级高的进程可以随时抢占进来，打断别人，强行插队。但是，抢占会引起上下文切换，上下文切换的时间本身对吞吐率来讲，是一个消耗，这个消耗可以低到2us或者更低，但是上下文切换更大的消耗不是切换本身，而是切换会引起大量的cache miss。

##### 上下文切换更大的消耗不是切换本身，而是切换会引起大量的cache miss

##### 不抢肯定响应差，抢了吞吐会下降

##### Linux有三类区间是不可以抢占调度的，这三类区间是：

###### 中断 软中断 持有类似spin_lock这样的锁而锁住该CPU核调度的情况

#### CPU消耗型 vs. I/O消耗型

##### 在Linux运行的进程，分为2类，一类是CPU消耗型（狂算），一类是I/O消耗型（狂睡，等I/O）， 前者CPU利用率高，后者CPU利用率低。 一般而言，I/O消耗型任务对延迟比较敏感，应该被优先调度

##### Linux的进程，对于RT进程而言，按照SCHED_FIFO和SCHED_RR的策略，优先级高先执行；优先级高的睡眠了后优先级的执行；同等优先级的SCHED_FIFO先ready的跑到睡，后ready的接着跑；而同等优先级的RR则进行时间片轮转。

##### RT的进程调度有一点“恶霸”色彩，我高优先级的没睡，低优先级的你就靠边站。 但是Linux的绝大多数进程都不是RT的进程，而是采用SCHED_NORMAL策略（这符合蜘蛛侠法则）。 NORMAL的人比较善良，我们一般用nice来形容它们的优先级，nice越高，优先级越低（你越nice，就越喜欢在地铁让座，当然越坐不到座位）。 普通进程的跑法，并不是nice低的一定堵着nice高的（要不然还说什么“善良”），它是按照如下公式进行： vruntime = pruntime * NICE_0_LOAD/ weight 其中NICE_0_LOAD是1024，也就是NICE是0的进程的weight。vruntime是进程的虚拟运行时间，pruntime是物理运行时间，weight是权重，权重完全由nice决定

##### 在RT进程都睡过去之后（有一个特例就是RT没睡也会跑普通进程，那就是RT加起来跑地实在太久太久，普通进程必须喝点汤了），Linux开始跑NORMAL的，它倾向于调度vruntime（虚拟运行时间）最小的普通进程，根据我们小学数学知识，vruntime要小，要么分子小（喜欢睡，I/O型进程，pruntime不容易长大），要么分母大（nice值低，优先级高，权重大）。这样一个简单的公式，就同时照顾了普通进程的优先级和CPU/IO消耗情况。

##### 普通进程的调度，是一个综合考虑你喜欢干活还是喜欢睡和你的nice值是多少的结果。

#### 分配vs. 占据

##### Linux作为一个把应用程序员当傻逼的操作系统，它必须允许应用程序犯错

##### malloc()成功的一刻，也不要以为真的拿到了内存，这个时候你的vss（虚拟地址空间，Virtual Set Size）会增大，但是你的rss(驻留在内存条上的内存，Resident SetSize)内存会随着写到每一页而缓慢增大。所以，分配成功的一刻，顶多只是被忽悠了，和你实际占有还是不占有，暂时没有半毛钱关系。

##### 系统出现out-of-memory，oom_score最大（最该死的）的进程被系统杀死



#### 隔离vs. 共享

##### Linux进程究竟耗费了多少内存，是一个非常复杂的概念，除了上面的vss, rss外，还有pss和uss，这些都是Linux不同于RTOS的显著特点之一。Linux各个进程既要做到隔离，但是隔离中又要实现共享，比如1000个进程都用libc，libc的代码段显然在内存只应该有一份



##### 仅从此图而言，进程1044的vss和rss分别是： vss= 1+2+3 rss= 4+5+6 但是是不是“4+5+6”就是1044这个进程耗费的内存呢？这显然也是不准确的，因为4明显被3个进程指向，5明显被2个进程指向，坏事是大家一起干的，不能1044一个人背黑锅。这个时候，就衍生出了一个pss（按比例计算的驻留内存, Proportional Set Size ）的概念，仅从这一幅图而言，进程1044的pss为： rss= 4/3 +5/2 +6 最后，还有进程1044独占且驻留的内存uss（Unique Set Size ），仅从此图而言， Uss = 6。

## CPU

### 寄存器是CPU里的东西,内存是挂在CPU外面的数据总线上的,访问内存时要在CPU的寄存器填上地址,再执行相应的汇编指令,这时CPU会在数据总线上生成读取或写入内存数据的时钟信号,最终内存的内容会被CPU寄存器的内容更新(写入)或被读入CPU的寄存器(读取) 不只是PC上的CPU,所有的嵌入式CPU,单片机都一个样

### 寄存器属于CPU的一个组成部分而缓存只是集成到CPU封装内完全是和CPU独立的器件。另外二者速度相差很大，寄存器存取速度最快 其次缓存最后是内存。

### 程序在执行过程中通常有用户态和内核态两种状态，CPU对处于内核态根据上下文环境进一步细分，因此有了下面三种状态： （1）内核态，运行于进程上下文，内核代表进程运行于内核空间。 （2）内核态，运行于中断上下文，内核代表硬件运行于内核空间。 （3）用户态，运行于用户空间。

### 一个进程的上下文可以分为三个部分:用户级上下文、寄存器上下文以及系统级上下文。 （1）用户级上下文: 正文、数据、用户堆栈以及共享存储区； （2）寄存器上下文: 通用寄存器、程序寄存器(IP)、处理器状态寄存器(EFLAGS)、栈指针(ESP)； （3）系统级上下文: 进程控制块task_struct、内存管理信息(mm_struct、vm_area_struct、pgd、pte)、内核栈。

### 当发生进程调度时，进行进程切换就是上下文切换(context switch).操作系统必须对上面提到的全部信息进行切换，新调度的进程才能运行。而系统调用进行的模式切换(mode switch)。模式切换与进程切换比较起来，容易很多，而且节省时间，因为模式切换最主要的任务只是切换进程寄存器上下文的切换。

### 当前进程因时间片用完或者因等待某个事件而阻塞时，进程调度需要把处理器的使用权从当前进程交给另一个进程，这个过程叫做进程切换

## 内存

### 变量在内存中的布局

#### 变量（函数外）：如果未初始化，则存放在BSS段；否则存放在data段

#### 变量（函数内）：如果没有指定static修饰符，则存放在栈中；否则同上

#### 常量：存放在文本段.text

#### 函数参数：存放在栈或寄存器中

### 内存可以分为以下几段：

#### 文本段：包含实际要执行的代码（机器指令）和常量。它通常是共享的，多个实例之间共享文本段。文本段是不可修改的。

#### 初始化数据段：包含程序已经初始化的全局变量，.data。

#### 未初始化数据段：包含程序未初始化的全局变量，.bbs。该段中的变量在执行之前初始化为0或NULL。

#### 栈：由系统管理，由高地址向低地址扩展。

#### 堆：动态内存，由用户管理。通过malloc/alloc/realloc、new/new[]申请空间，通过free、delete/delete[]释放所申请的空间。由低地址想高地址扩展。

### 堆栈区别



## 缓存

### 缓存失效：

#### 　　引起这个原因的主要因素是高并发下，我们一般设定一个缓存的过期时间时，可能有一些会设置5分钟啊，10分钟这些；并发很高时可能会出在某一个时间同时生成了很多的缓存，并且过期时间在同一时刻，这个时候就可能引发——当过期时间到后，这些缓存同时失效，请求全部转发到DB，DB可能会压力过重。

#### 　　处理方法： 　　　　一个简单方案就是将缓存失效时间分散开，不要所以缓存时间长度都设置成5分钟或者10分钟；比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 缓存失效时产生的雪崩效应，将所有请求全部放在数据库上，这样很容易就达到数据库的瓶颈，导致服务无法正常提供。尽量避免这种场景的发生。

### 缓存穿透：

#### 　　出现场景：指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。 　　当在流量较大时，出现这样的情况，一直请求DB，很容易导致服务挂掉。

#### 处理方法： 　　　　方法1.在封装的缓存SET和GET部分增加个步骤，如果查询一个KEY不存在，就已这个KEY为前缀设定一个标识KEY；以后再查询该KEY的时候，先查询标识KEY，如果标识KEY存在，就返回一个协定好的非false或者NULL值，然后APP做相应的处理，这样缓存层就不会被穿透。当然这个验证KEY的失效时间不能太长。 　　　　方法2.如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，一般只有几分钟。 　　　　方法3.采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。

### 缓存并发

#### 　　出现场景：当网站并发访问高，一个缓存如果失效，可能出现多个进程同时查询DB，同时设置缓存的情况，如果并发确实很大，这也可能造成DB压力过大，还有缓存频繁更新的问题。

#### 　　处理方法：对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询。

### 缓存更新的套路

#### Cache Aside Pattern

##### 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。

##### 例外

###### 一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

###### 要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。

#### Read/Write Through Pattern

##### Read Through

###### Read Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则用缓存服务自己来加载，从而对应用方是透明的。

##### Write Through

###### Write Through 套路和Read Through相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作）



#### Write Behind Caching Pattern

##### 在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。

##### 问题

###### 数据不是强一致性的，而且可能会丢失



## 上下文切换

### 上下文交换(英语：context switch)，又称环境切换，电脑术语，是一个储存和重建CPU的状态 (内文)，因此令多个进程(process)可以分享单一CPU资源的计算过程。要交换CPU上的进程时，必需先行储存目前进程的状态，再将欲执行的进程之状态读回CPU中。[1]

### 有三种可能的情况会发生上下文交换，分别为：

### 多工 最常见的，在一些排程(scheduling)算法内，其中行程有时候需要暂时离开CPU，让另一个行程进来CPU运作。在先占式多工系统中，每一个行程都将轮流执行不定长度的时间，这些时间段落称为时间片。如果行程并非自愿让出CPU(例如执行I/O操作时，行程就需放弃CPU使用权)，当时限到时，系统将产生一个定时中断，操作系统将排定由其它的行程来执行。此机制用以确保CPU不致被较依赖处理器运算的行程垄断。若无定时中断，除非行程自愿让出CPU，否则该行程将持续执行。对于拥有较多I/O指令的行程，往往执行不了多久，便需要让出CPU；而较依赖处理器的行程相对而言I/O操作较少，反而能一直持续使用CPU，便形成了垄断现象。此即Convoy效应。

### 中断处理 在接受到中断（Interrupt）的时候，CPU必须要进行上下文交换。

### 用户态或者内核态的交换 当用户态和内核态交换发生的时候，并不需要进行上下文交换；并且用户态和kernel mode的交换本身并不是一个上下文交换。不过，根据操作系统的不同，有时候会在此时进行一次上下文交换的步骤。

## IPC

### 进程间通信技术包括消息传递、同步、共享内存和远程过程调用。IPC是一种标准的Unix通信机制。

### 使用IPC 的理由： 信息共享：Web服务器，通过网页浏览器使用进程间通信来共享web文件（网页等）和多媒体； 加速：维基百科使用通过进程间通信进行交流的多服务器来满足用户的请求； 模块化; 私有权分离.

### 与直接共享内存地址空间的多线程编程相比，IPC的缺点：[1] 采用了某种形式的内核开销，降低了性能; 几乎大部分IPC都不是程序设计的自然扩展，往往会大大地增加程序的复杂度。

### 消息队列

#### 消息队列提供了异步的通信协议，每一个贮列中的纪录包含详细说明的数据，包含发生的时间，输入设备的种类，以及特定的输入参数，也就是说：消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。[1]

#### 优缺点 消息队列本身是异步的，它允许接收者在消息发送很长时间后再取回消息，这和大多数通信协议是不同的。例如WWW中使用的HTTP协议（HTTP/2之前）是同步的，因为客户端在发出请求后必须等待服务器回应。然而，很多情况下我们需要异步的通信协议。比如，一个进程通知另一个进程发生了一个事件，但不需要等待回应。但消息队列的异步特点，也造成了一个缺点，就是接收者必须轮询消息队列，才能收到最近的消息。

### BSD socket & IPC socket

## 按资源类型划分系统

### 系统的类型反映了系统的主要瓶颈。

### IO密集型

#### 现实情况中，大部分系统在由小变大的过程中，最先出现瓶颈的是IO。 高并发，存储介质的读写（例如数据库，磁盘等）

### 计算密集型

#### 随着业务逻辑的复杂化，接下来出现瓶颈的是计算，也就是常说的CPU idle不足。 出现计算瓶颈的时候，一般会使用水平扩展（加机器）和垂直扩张（服务拆分）两个方法。

### 数据密集型

#### 随着数据量（用户数量，客户数量）的增长，再接下来出现瓶颈的是内存。 内存的合理使用比以往更加重要。一方面，大数据理论已经非常普及，用数据驱动产品也已经被普遍接受并落地，同时数据分析也促使产品设计的更加精细，因此系统承载的数量比以前有了很大的变化，系统遇到内存瓶颈的时间也比以前大大缩短了。另一方面，内存依然是相对昂贵的硬件，不能无限制的使用。

# 底层

## 内存管理

### 进程的内存

#### linux没有为堆栈分配静态的大小，而是利用缺页中断使得堆栈在运行期动态增长，当然没有了固定的大小也就不存在溢出的问题了，只要虚拟内存足够，动态增长的需求就有可能被满足

#### 从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。 1、brk是将数据段(.data)的最高地址指针_edata往高地址推； 2、mmap是在进程的虚拟地址空间中（堆和栈中间，称为文件映射区域的地方）找一块空闲的虚拟内存。 这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系。

#### 段页式

##### 碎片问题

###### 内部碎片：已经分配的内存，却不能被利用的内存空间； 缘由：所有内存分配必须起始可被4、8或16(体系结构决定)整除的地址或者MMU分页机制限制； 解决方案：slab分配器有所改善 实例：请求一个11Byte的内存块，系统可能会分配12Byte、16Byte等稍大一些的字节，这些多余空间就产生碎片

###### 外部碎片：未被分配的内存，由于太多零碎的不连续小内存，无法满足当前较大内存的申请要求； 原因：频繁的分配与回收物理页导致大量的小块内存夹杂在已分配页面中间； 解决方案：伙伴算法有所改善

##### 进程内存分段使用虚拟内存

###### 页表的问题： 1. 共享困难 2. 程序地址空间受限于虚拟地址

###### 对于普通进程对应的内存空间包含5种不同的数据区： 代码段 数据段(初始化的数据) BSS段(未初始化的数据) 堆：动态分配的内存段，大小不固定，可动态扩张(malloc等函数分配内存)，或动态缩减(free等函数释放)； 栈：存放临时创建的局部变量；

##### 物理内存是通过分页机制实现的

###### 用 VPN 来做页表索引，也就是说页表的大小为虚拟地址位数 / 页的大小 页表太大，使用多级页表解决

###### 页表是在内存中，而 MMU 位于 CPU 芯片中，这样每次地址翻译可能都需要先访问一次内存中的页表（CPU L1,L2,L3 Cache Miss 的时候访问内存），效率非常低下 对应的解决方案是引入页表的高速缓存：TLB（Translation Lookaside Buffer）

##### 页表

###### 应用程序操作的对象时映射到物理内存之上的虚拟内存，而处理器直接操作的是物理内存。故应用程序访问一个虚拟地址时，需要将虚拟地址转换为物理地址，然后处理器才能解析地址访问请求，这个转换工作通过查询页表完成。 Linux使用三级页表完成地址转换



###### 多数体系结构，搜索页表工作由硬件完成。每个进程都有自己的页表(线程会共享页表)。为了加快搜索，实现了翻译后缓冲器(TLB)，作为将虚拟地址映射到物理地址的硬件缓存。还有写时拷贝方式共享页表，当fork()时，父子进程共享页表，只有当子进程或父进程试图修改特定页表项时，内核才创建该页表项的新拷贝，之后父子进程不再共享该页表项。可见，利用共享页表可以消除fork()操作中页表拷贝所带来的消耗。

#### 内存布局

##### 动态栈增长是唯一一种访问未映射内存区域（图中白色区域）而被允许的情形。其它任何对未映射内存区域的访问都会触发页故障，从而导致段错误。一些被映射的区域是只读的，因此企图写这些区域也会导致段错误。



#### 堆栈区别



### linux内存管理

#### 内存管理架构



#### [地址映射](图：左中) linux内核使用页式内存管理，应用程序给出的内存地址是虚拟地址，它需要经过若干级页表一级一级的变换，才变成真正的物理地址。 想一下，地址映射还是一件很恐怖的事情。当访问一个由虚拟地址表示的内存空间时，需要先经过若干次的内存访问，得到每一级页表中用于转换的页表项（页表是存放在内存里面的），才能完成映射。也就是说，要实现一次内存访问，实际上内存被访问了N+1次（N=页表级数），并且还需要做N次加法运算。 所以，地址映射必须要有硬件支持，mmu（内存管理单元）就是这个硬件。并且需要有cache来保存页表，这个cache就是TLB（Translation lookaside buffer）。 尽管如此，地址映射还是有着不小的开销。假设cache的访存速度是内存的10倍，命中率是40%，页表有三级，那么平均一次虚拟地址访问大概就消耗了两次物理内存访问的时间。 于是，一些嵌入式硬件上可能会放弃使用mmu，这样的硬件能够运行VxWorks（一个很高效的嵌入式实时操作系统）、linux（linux也有禁用mmu的编译选项）、等系统。 但是使用mmu的优势也是很大的，最主要的是出于安全性考虑。各个进程都是相互独立的虚拟地址空间，互不干扰。而放弃地址映射之后，所有程序将运行在同一个地址空间。于是，在没有mmu的机器上，一个进程越界访存，可能引起其他进程莫名其妙的错误，甚至导致内核崩溃。 在地址映射这个问题上，内核只提供页表，实际的转换是由硬件去完成的。那么内核如何生成这些页表呢？这就有两方面的内容，虚拟地址空间的管理和物理内存的管理。（实际上只有用户态的地址映射才需要管理，内核态的地址映射是写死的。）

#### [虚拟地址管理](图：左下) 每个进程对应一个task结构，它指向一个mm结构，这就是该进程的内存管理器。（对于线程来说，每个线程也都有一个task结构，但是它们都指向同一个mm，所以地址空间是共享的。） mm->pgd指向容纳页表的内存，每个进程有自已的mm，每个mm有自己的页表。于是，进程调度时，页表被切换（一般会有一个CPU寄存器来保存页表的地址，比如X86下的CR3，页表切换就是改变该寄存器的值）。所以，各个进程的地址空间互不影响（因为页表都不一样了，当然无法访问到别人的地址空间上。但是共享内存除外，这是故意让不同的页表能够访问到相同的物理地址上）。 用户程序对内存的操作（分配、回收、映射、等）都是对mm的操作，具体来说是对mm上的vma（虚拟内存空间）的操作。这些vma代表着进程空间的各个区域，比如堆、栈、代码区、数据区、各种映射区、等等。 用户程序对内存的操作并不会直接影响到页表，更不会直接影响到物理内存的分配。比如malloc成功，仅仅是改变了某个vma，页表不会变，物理内存的分配也不会变。 假设用户分配了内存，然后访问这块内存。由于页表里面并没有记录相关的映射，CPU产生一次缺页异常。内核捕捉异常，检查产生异常的地址是不是存在于一个合法的vma中。如果不是，则给进程一个"段错误"，让其崩溃；如果是，则分配一个物理页，并为之建立映射。

#### [物理内存管理](图：右上) 那么物理内存是如何分配的呢？ 首先，linux支持NUMA（非均质存储结构），物理内存管理的第一个层次就是介质的管理。pg_data_t结构就描述了介质。一般而言，我们的内存管理介质只有内存，并且它是均匀的，所以可以简单地认为系统中只有一个pg_data_t对象。 每一种介质下面有若干个zone。一般是三个，DMA、NORMAL和HIGH。 DMA：因为有些硬件系统的DMA总线比系统总线窄，所以只有一部分地址空间能够用作DMA，这部分地址被管理在DMA区域（这属于是高级货了）； HIGH：高端内存。在32位系统中，地址空间是4G，其中内核规定3~4G的范围是内核空间，0~3G是用户空间（每个用户进程都有这么大的虚拟空间）（图：中下）。前面提到过内核的地址映射是写死的，就是指这3~4G的对应的页表是写死的，它映射到了物理地址的0~1G上。（实际上没有映射1G，只映射了896M。剩下的空间留下来映射大于1G的物理地址，而这一部分显然不是写死的）。所以，大于896M的物理地址是没有写死的页表来对应的，内核不能直接访问它们（必须要建立映射），称它们为高端内存（当然，如果机器内存不足896M，就不存在高端内存。如果是64位机器，也不存在高端内存，因为地址空间很大很大，属于内核的空间也不止1G了）； NORMAL：不属于DMA或HIGH的内存就叫NORMAL。 在zone之上的zone_list代表了分配策略，即内存分配时的zone优先级。一种内存分配往往不是只能在一个zone里进行分配的，比如分配一个页给内核使用时，最优先是从NORMAL里面分配，不行的话就分配DMA里面的好了（HIGH就不行，因为还没建立映射），这就是一种分配策略。 每个内存介质维护了一个mem_map，为介质中的每一个物理页面建立了一个page结构与之对应，以便管理物理内存。 每个zone记录着它在mem_map上的起始位置。并且通过free_area串连着这个zone上空闲的page。物理内存的分配就是从这里来的，从 free_area上把page摘下，就算是分配了。（内核的内存分配与用户进程不同，用户使用内存会被内核监督，使用不当就"段错误"；而内核则无人监督，只能靠自觉，不是自己从free_area摘下的page就不要乱用。）

#### [建立地址映射] 内核需要物理内存时，很多情况是整页分配的，这在上面的mem_map中摘一个page下来就好了。比如前面说到的内核捕捉缺页异常，然后需要分配一个page以建立映射。 说到这里，会有一个疑问，内核在分配page、建立地址映射的过程中，使用的是虚拟地址还是物理地址呢？首先，内核代码所访问的地址都是虚拟地址，因为CPU指令接收的就是虚拟地址（地址映射对于CPU指令是透明的）。但是，建立地址映射时，内核在页表里面填写的内容却是物理地址，因为地址映射的目标就是要得到物理地址。 那么，内核怎么得到这个物理地址呢？其实，上面也提到了，mem_map中的page就是根据物理内存来建立的，每一个page就对应了一个物理页。 于是我们可以说，虚拟地址的映射是靠这里page结构来完成的，是它们给出了最终的物理地址。然而，page结构显然是通过虚拟地址来管理的（前面已经说过，CPU指令接收的就是虚拟地址）。那么，page结构实现了别人的虚拟地址映射，谁又来实现page结构自己的虚拟地址映射呢？没人能够实现。 这就引出了前面提到的一个问题，内核空间的页表项是写死的。在内核初始化时，内核的地址空间就已经把地址映射写死了。page结构显然存在于内核空间，所以它的地址映射问题已经通过“写死”解决了。 由于内核空间的页表项是写死的，又引出另一个问题，NORMAL（或DMA）区域的内存可能被同时映射到内核空间和用户空间。被映射到内核空间是显然的，因为这个映射已经写死了。而这些页面也可能被映射到用户空间的，在前面提到的缺页异常的场景里面就有这样的可能。映射到用户空间的页面应该优先从HIGH区域获取，因为这些内存被内核访问起来很不方便，拿给用户空间再合适不过了。但是HIGH区域可能会耗尽，或者可能因为设备上物理内存不足导致系统里面根本就没有HIGH区域，所以，将NORMAL区域映射给用户空间是必然存在的。 但是NORMAL区域的内存被同时映射到内核空间和用户空间并没有问题，因为如果某个页面正在被内核使用，对应的page应该已经从free_area被摘下，于是缺页异常处理代码中不会再将该页映射到用户空间。反过来也一样，被映射到用户空间的page自然已经从free_area被摘下，内核不会再去使用这个页面。

#### [内核空间管理](图：右下) 除了对内存整页的使用，有些时候，内核也需要像用户程序使用malloc一样，分配一块任意大小的空间。这个功能是由slab系统来实现的。 slab相当于为内核中常用的一些结构体对象建立了对象池，比如对应task结构的池、对应mm结构的池、等等。 而slab也维护有通用的对象池，比如"32字节大小"的对象池、"64字节大小"的对象池、等等。内核中常用的kmalloc函数（类似于用户态的malloc）就是在这些通用的对象池中实现分配的。 slab除了对象实际使用的内存空间外，还有其对应的控制结构。有两种组织方式，如果对象较大，则控制结构使用专门的页面来保存；如果对象较小，控制结构与对象空间使用相同的页面。 除了slab，linux 2.6还引入了mempool（内存池）。其意图是：某些对象我们不希望它会因为内存不足而分配失败，于是我们预先分配若干个，放在mempool中存起来。正常情况下，分配对象时是不会去动mempool里面的资源的，照常通过slab去分配。到系统内存紧缺，已经无法通过slab分配内存时，才会使用 mempool中的内容。

#### [页面换入换出](图：左上)(图：右上) 页面换入换出又是一个很复杂的系统。内存页面被换出到磁盘，与磁盘文件被映射到内存，是很相似的两个过程（内存页被换出到磁盘的动机，就是今后还要从磁盘将其载回内存）。所以swap复用了文件子系统的一些机制。 页面换入换出是一件很费CPU和IO的事情，但是由于内存昂贵这一历史原因，我们只好拿磁盘来扩展内存。但是现在内存越来越便宜了，我们可以轻松安装数G的内存，然后将swap系统关闭。于是swap的实现实在让人难有探索的欲望，在这里就不赘述了。（另见：《linux内核页面回收浅析》）

#### [用户空间内存管理] malloc是libc的库函数，用户程序一般通过它（或类似函数）来分配内存空间。 libc对内存的分配有两种途径，一是调整堆的大小，二是mmap一个新的虚拟内存区域（堆也是一个vma）。 在内核中，堆是一个一端固定、一端可伸缩的vma（图：左中）。可伸缩的一端通过系统调用brk来调整。libc管理着堆的空间，用户调用malloc分配内存时，libc尽量从现有的堆中去分配。如果堆空间不够，则通过brk增大堆空间。 当用户将已分配的空间free时，libc可能会通过brk减小堆空间。但是堆空间增大容易减小却难，考虑这样一种情况，用户空间连续分配了10块内存，前9块已经free。这时，未free的第10块哪怕只有1字节大，libc也不能够去减小堆的大小。因为堆只有一端可伸缩，并且中间不能掏空。而第10块内存就死死地占据着堆可伸缩的那一端，堆的大小没法减小，相关资源也没法归还内核。 当用户malloc一块很大的内存时，libc会通过mmap系统调用映射一个新的vma。因为对于堆的大小调整和空间管理还是比较麻烦的，重新建一个vma会更方便（上面提到的free的问题也是原因之一）。 那么为什么不总是在malloc的时候去mmap一个新的vma呢？第一，对于小空间的分配与回收，被libc管理的堆空间已经能够满足需要，不必每次都去进行系统调用。并且vma是以page为单位的，最小就是分配一个页；第二，太多的vma会降低系统性能。缺页异常、vma的新建与销毁、堆空间的大小调整、等等情况下，都需要对vma进行操作，需要在当前进程的所有vma中找到需要被操作的那个（或那些）vma。vma数目太多，必然导致性能下降。（在进程的vma较少时，内核采用链表来管理vma；vma较多时，改用红黑树来管理。）

#### [用户的栈] 与堆一样，栈也是一个vma（图：左中），这个vma是一端固定、一端可伸（注意，不能缩）的。这个vma比较特殊，没有类似brk的系统调用让这个vma伸展，它是自动伸展的。 当用户访问的虚拟地址越过这个vma时，内核会在处理缺页异常的时候将自动将这个vma增大。内核会检查当时的栈寄存器（如：ESP），访问的虚拟地址不能超过ESP加n（n为CPU压栈指令一次性压栈的最大字节数）。也就是说，内核是以ESP为基准来检查访问是否越界。 但是，ESP的值是可以由用户态程序自由读写的，用户程序如果调整ESP，将栈划得很大很大怎么办呢？内核中有一套关于进程限制的配置，其中就有栈大小的配置，栈只能这么大，再大就出错。 对于一个进程来说，栈一般是可以被伸展得比较大（如：8MB）。然而对于线程呢？ 首先线程的栈是怎么回事？前面说过，线程的mm是共享其父进程的。虽然栈是mm中的一个vma，但是线程不能与其父进程共用这个vma（两个运行实体显然不用共用一个栈）。于是，在线程创建时，线程库通过mmap新建了一个vma，以此作为线程的栈（大于一般为：2M）。 可见，线程的栈在某种意义上并不是真正栈，它是一个固定的区域，并且容量很有限。

### windows内存管理

#### windows的内存管理很是严谨，使用内存必须首先分配，当然每个操作系统都是这样，然而windows的严谨在于分配的过程，分为保留和提交两个阶段，其中保留的含义就是在进程的虚拟地址空间保留一块空间，不能用作他用，保留的概念是针对虚拟地址空间的，而提交的含义是将刚才保留的虚拟地址空间的虚拟内存块映射到物理内存，

#### windows中堆栈的分配是静态的，也就是说在PE文件中确定了线程堆栈的大小并且一般不能在运行时动态改变。 首先为你的堆栈确定一个大小，然后将这段如此大小的内存块的第一个和最后一个页面设置为保留，堆栈向下增长，windows将依次把正在被使用的下一个页面设置为保护提交，每当保护提交页面被访问时系统会得到通知，注意得到通知而不是出错信息，并没有什么严重的错误，因为保护提交页面可以被访问，它已经提交了，只不过由于具有保护属性，所有要告知系统这一件事，系统得知后可以将保护提交属性设置给后一个页面，依次类推，线程的堆栈空间页面将按照从高到底的顺序一个个被提交，而紧接着被提交的页面将被设置为保护提交，直到最后，到达堆栈的末尾的时候，windows会检测到，此时不再将最后一个页面设置为保护提交，而是引发一个栈溢出异常。

## 堆栈寄存器

### eax 返回值寄存器，用于存储函数返回值 eip 当前指令地址寄存器，其值为内存中的指令地址，只能通过jmp, call, ret等修改，不可直接修改 ebp 堆栈底端地址寄存器，其值对应内存中的堆栈底端 esp 堆栈顶端地址寄存器，其值对应内存中的堆栈顶端，ebp-esp之间即堆栈内容

#### pushl %A等价于： subl $4, %esp ;堆栈顶端指针向下移动4字节（即32位） movl %A, (%esp) ;将寄存器A的内容存到寄存器esp对应的内存地址

#### popl %A等价于： movl (%esp), %A ;将堆栈顶端对应的内存中的内容幅值给A addl $4, %esp ;堆栈顶端上移4字节

#### call f 等价于如下汇编代码： pushl %eip ;将当前的程序位置压栈 movl f, %eip ;将f函数的入口位置赋给eip，则下一条指令执行f的入口指令。

#### ret 等价于如下汇编代码： popl %eip

#### enter指令等价于如下汇编指令： pushl %ebp movl %esp, %ebp

#### leave指令则等价于： movl %ebp, %esp popl %ebp

### 1. 在enter之后，形成了一个新的堆栈，ebp成为原函数堆栈和被调用函数堆栈的分界点。ebp的正前方4(%ebp)是前一个函数的运行位置指针，再往前8(%ebp)即为函数参数的位置。 2. 由于leave语句的存在，函数堆栈可以直接恢复到函数调用前的状态，不需要依次进行弹栈。

### 风格



## ELF

### ELF文件主要有三种，分别是： * 可重定位的目标文件，在编译时用gcc的-c参数时产生。 * 可执行文件，这类文件就是我们后面要讨论的可以执行的文件。 * 共享库，这里主要是动态共享库，而静态共享库则是可重定位的目标文件通过ar命令 组织的

### ELF文件的主要节区(section)有.data,.text,.bss,.interp等，而主要段(segment)有LOAD,INTERP 等

### .data 初始化的数据 比如int a=10 .bss 未初始化的数据 比如char sum[100];这个在程序执行之前，内核将初始化为0 .text 程序代码正文 即可执行指令集 .interp 描述程序需要的解释器（动态连接和装载程序） 存有解释器的全路径，如/lib/ld-linux.so 而程序在执行以后，.data, .bss,.text等一些节区会被Program header table映射到LOAD 段，.interp则被映射到了INTERP段

## 汇编指令

### 数据传送指令

#### 这部分指令包括通用数据传送指令MOV、条件传送指令CMOVcc、堆栈操作指令PUSH/PUSHA/PUSHAD/POP/POPA/POPAD、交换指令XCHG/XLAT/BSWAP、地址或段描述符选择子传送指令LEA/LDS/LES/LFS/LGS/LSS等。

### 整数和逻辑运算指令

#### 这部分指令用于执行算术和逻辑运算，包括加法指令ADD/ADC、减法指令SUB/SBB、加一指令INC、减一指令DEC、比较操作指令CMP、乘法指令MUL/IMUL、除法指令DIV/IDIV、符号扩展指令CBW/CWDE/CDQE、十进制调整指令DAA/DAS/AAA/AAS、逻辑运算指令NOT/AND/OR/XOR/TEST等

### 移位指令

#### 部分指令用于将寄存器或内存操作数移动指定的次数。包括逻辑左移指令SHL、逻辑右移指令SHR、算术左移指令SAL、算术右移指令SAR、循环左移指令ROL、循环右移指令ROR等。

### 位操作指令

#### 这部分指令包括位测试指令BT、位测试并置位指令BTS、位测试并复位指令BTR、位测试并取反指令BTC、位向前扫描指令BSF、位向后扫描指令BSR等。

### 条件设置指令

#### 这不是一条具体的指令，而是一个指令簇，包括大约30条指令，用于根据EFLAGS寄存器的某些位状态来设置一个8位的寄存器或者内存操作数。比如SETE/SETNE/SETGE等等

### 控制转移指令

#### JMP( JuMP ) 无条件转移指令

#### 条件转移指令

#### 比较两个无符号数，并根据比较的结果转移

#### 比较两个带符号数，并根据比较的结果转移

#### 测试CX的值为0则转移指令

#### 循环指令

#### 过程调用及返回指令

#### 这部分包括无条件转移指令JMP、条件转移指令Jcc/JCXZ、循环指令LOOP/LOOPE/LOOPNE、过程调用指令CALL、子过程返回指令RET、中断指令INTn、INT3、INTO、IRET等。注意，Jcc是一个指令簇，包含了很多指令，用于根据EFLAGS寄存器的某些位状态来决定是否转移；INT n是软中断指令，n可以是0到255之间的数，用于指示中断向量号

### 串处理指令

#### 这部分指令用于对数据串进行操作，包括串传送指令MOVS、串比较指令CMPS、串扫描指令SCANS、串加载指令LODS、串保存指令STOS，这些指令可以有选择地使用REP/REPE/REPZ/REPNE和REPNZ的前缀以连续操作。

### 其他

#### 输入输出指令

##### 这部分指令用于同外围设备交换数据，包括端口输入指令IN/INS、端口输出指令OUT/OUTS。

#### 高级语言辅助指令

##### 这部分指令为高级语言的编译器提供方便，包括创建栈帧的指令ENTER和释放栈帧的指令LEAVE。

#### 控制和特权指令

##### 这部分包括无操作指令NOP、停机指令HLT、等待指令WAIT/MWAIT、换码指令ESC、总线封锁指令LOCK、内存范围检查指令BOUND、全局描述符表操作指令LGDT/SGDT、中断描述符表操作指令LIDT/SIDT、局部描述符表操作指令LLDT/SLDT、描述符段界限值加载指令LSR、描述符访问权读取指令LAR、任务寄存器操作指令LTR/STR、请求特权级调整指令ARPL、任务切换标志清零指令CLTS、控制寄存器和调试寄存器数据传送指令MOV、高速缓存控制指令INVD/WBINVD/INVLPG、型号相关寄存器读取和写入指令RDMSR/WRMSR、处理器信息获取指令CPUID、时间戳读取指令RDTSC等。

#### 浮点和多媒体指令

##### 这部分指令用于加速浮点数据的运算，以及用于加速多媒体数据处理的单指令多数据（SIMD及其扩展SSEx）指令。这部分指令数据非常庞大，无法一一列举，请自行参考INTEL手册。

#### 虚拟机扩展指令

##### 这部分指令包括INVEPT/INVVPID/VMCALL/VMCLEAR/VMLAUNCH/VMRESUME/VMPTRLD/VMPTRST/VMREAD/VMWRITE/VMXOFF/VMON等

# TCP

## 特点

### TCP通过校验和、序列号、确认应答、重发控制、连接管理和窗口控制等机制实现可靠性传输

### 面向连接的、可靠的流协议

### 顺序控制、重复控制、流量控制、拥塞控制、提高网络利用率的等功能

### 通信识别：源IP、目的IP、协议号、源port、目的port

## 为什么多 TCP 连接分块下载比单连接下载快？

### TCP特性使得每个TCP连接可以得到均等的带宽。在多用户环境下，一个用户拥有越多TCP连接，获得的带宽越大。

### 就是tcp的拥塞避免机制完全不适用于现有的网络条件了。设计tcp的年代，网络带宽很低，所以tcp被设计成一个极度友好的协议，一旦发现拥塞就拼命退让，但是现在的网络已经带宽极大改善了，而网络质量反而极大降低，延迟也大了很多（滑动窗口在大延迟下成了一个很逗比的存在），所以当tcp进行拥塞避免的时候，其实网络根本就没有拥塞，只是质量不好丢了点包而已。结果就是tcp没有办法最大化的利用带宽。

## 三次握手四次断开

### 这是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的建连请求后，它可以把ACK和SYN（ACK起应答作用，而SYN起同步作用）放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。

## 为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态

### 什么是2MSL？MSL即Maximum Segment Lifetime，也就是报文最大生存时间，引用《TCP/IP详解》中的话：“它(MSL)是任何报文段被丢弃前在网络内的最长时间。”那么，2MSL也就是这个时间的2倍，当TCP连接完成四个报文段的交换时，主动关闭的一方将继续等待一定时间(2-4分钟)，即使两端的应用程序结束。

### 第一，虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SEND状态到ESTABLISH状态那样）；但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的ACK报文会一定被对方收到，因此对方处于LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。

### 第二，报文可能会被混淆，意思是说，其他时候的连接可能会被当作本次的连接

## DDoS攻击

### DDoS(Distributed Denial of Service，分布式拒绝服务)攻击的主要目的是让指定目标无法提供正常服务，甚至从互联网上消失，是目前最强大、最难防御的攻击之一。这是一个世界级的难题并没有解决办法只能缓解

### 按照发起的方式，DDoS可以简单分为三类。

#### 第一类以力取胜，海量数据包从互联网的各个角落蜂拥而来，堵塞IDC入口，让各种强大的硬件防御系统、快速高效的应急流程无用武之地。这种类型的攻击典型代表是ICMP Flood和UDP Flood，现在已不常见。

#### 第二类以巧取胜，灵动而难以察觉，每隔几分钟发一个包甚至只需要一个包，就可以让豪华配置的服务器不再响应。这类攻击主要是利用协议或者软件的漏洞发起，例如Slowloris攻击、Hash冲突攻击等，需要特定环境机缘巧合下才能出现。

#### 第三类是上述两种的混合，轻灵浑厚兼而有之，既利用了协议、系统的缺陷，又具备了海量的流量，例如SYN Flood攻击、DNS Query Flood攻击，是当前的主流攻击方式。

### 类型

#### DNS Query Flood就是攻击者操纵大量傀儡机器，对目标发起海量的域名查询请求。为了防止基于ACL的过滤，必须提高数据包的随机性。常用的做法是UDP层随机伪造源IP地址、随机伪造源端口等参数。在DNS协议层，随机伪造查询ID以及待解析域名。随机伪造待解析域名除了防止过滤外，还可以降低命中DNS缓存的可能性，尽可能多地消耗DNS服务器的CPU资源 DNS可以分为普通DNS和授权域DNS，攻击普通DNS，IP地址需要随机伪造，并且指明服务器要求做递归解析;但攻击授权域DNS，伪造的源IP地址则不应该是纯随机的，而应该是事先收集的全球各地ISP的DNS地址，这样才能达到最大攻击效果，使流量清洗设备处于添加IP黑名单还是不添加IP黑名单的尴尬处境。添加会导致大量误杀，不添加黑名单则每个报文都需要反向探测从而加大性能压力

#### SYN-Flood不会完成TCP三次握手的第三步，也就是不发送确认连接的信息给服务器。这样，服务器无法完成第三次握手，但服务器不会立即放弃，服务器会不停的重试并等待一定的时间后放弃这个未完成的连接，这段时间叫做SYN timeout，这段时间大约30秒-2分钟左右。一个服务器若是处理这些大量的半连接信息而消耗大量的系统资源和网络带宽，这样服务器就不会再有空余去处理普通用户的正常请求 普通的SYN Flood容易被流量清洗设备通过反向探测、SYN Cookie等技术手段过滤掉，但如果在SYN Flood中混入SYN+ACK数据包，使每一个伪造的SYN数据包都有一个与之对应的伪造的客户端确认报文，这里的对应是指源IP地址、源端口、目的IP、目的端口、TCP窗口大小、TTL等都符合同一个主机同一个TCP Flow的特征，流量清洗设备的反向探测和SYN Cookie性能压力将会显著增大。其实SYN数据报文配合其他各种标志位，都有特殊的攻击效果，

#### 慢速连接攻击Slowloris HTTP协议规定，HTTP Request以\r\n\r\n结尾表示客户端发送结束，服务端开始处理。那么，如果永远不发送\r\n\r\n会如何?Slowloris就是利用这一点来做DDoS攻击的。攻击者在HTTP请求头中将Connection设置为Keep-Alive，要求Web Server保持TCP连接不要断开，随后缓慢地每隔几分钟发送一个key-value格式的数据到服务端，如a:b\r\n，导致服务端认为HTTP头部没有接收完成而一直等待。如果攻击者使用多线程或者傀儡机来做同样的操作，服务器的Web容器很快就被攻击者占满了TCP连接而不再接受新的请求。 很快的，Slowloris开始出现各种变种。比如POST方法向Web Server提交数据、填充一大大Content-Length但缓慢的一个字节一个字节的POST真正数据内容等等。

#### HTTP Flood的着重点，在于突破前端的cache，通过HTTP头中的字段设置直接到达Web Server本身。另外，HTTP Flood对目标的选取也非常关键，一般的攻击者会选择搜索之类需要做大量数据查询的页面作为攻击目标，这是非常正确的，可以消耗服务器尽可能多的资源。但这种攻击容易被清洗设备通过人机识别的方式识别出来，那么如何解决这个问题?很简单，尽量选择正常用户也通过APP访问的页面，一般来说就是各种Web API。正常用户和恶意流量都是来源于APP，人机差别很小，基本融为一体难以区分 攻击者并不需要控制大批的傀儡机，取而代之的是通过端口扫描程序在互联网上寻找匿名的HTTP代理或者SOCKS代理，攻击者通过匿名代理对攻击目标发起HTTP请求。匿名代理是一种比较丰富的资源，花几天时间获取代理并不是难事，因此攻击容易发起而且可以长期高强度的持续

#### CC攻击 互联网的架构追求扩展性本质上是为了提高并发能力，各种SQL性能优化措施：消除慢查询、分表分库、索引、优化数据结构、限制搜索频率等本质都是为了解决资源消耗，而CC大有反其道而行之的意味，占满服务器并发连接数，尽可能使请求避开缓存而直接读数据库，读数据库要找最消耗资源的查询，最好无法利用索引，每个查询都全表扫描，这样就能用最小的攻击资源起到最大的拒绝服务效果。

#### 来自P2P网络的攻击 互联网上的P2P用户和流量都是一个极为庞大的数字。如果他们都去一个指定的地方下载数据，使成千上万的真实IP地址连接过来，没有哪个设备能够支撑住 高级P2P攻击，是直接欺骗资源管理服务器。通过协议逆向，攻击者伪造出大批量的热门资源信息通过资源管理中心分发出去，瞬间就可以传遍整个P2P网络。更为恐怖的是，这种攻击是无法停止的，即使是攻击者自身也无法停止，攻击一直持续到P2P官方发现问题更新服务器且下载用户重启下载软件时为止

### 防御

#### DNS Flood防御 　　DNS攻击防御也有类似HTTP的防御手段，第一方案是缓存。其次是重发，可以是直接丢弃DNS报文导致UDP层面的请求重发，可以是返回特殊响应强制要求客户端使用TCP协议重发DNS查询请求。 　　特殊的，对于授权域DNS的保护，设备会在业务正常时期提取收到的DNS域名列表和ISP DNS IP列表备用，在攻击时，非此列表的请求一律丢弃，大幅降低性能压力。对于域名，实行同样的域名白名单机制，非白名单中的域名解析请求，做丢弃处理。

#### SYN Flood防御 　　前文描述过，SYN Flood攻击大量消耗服务器的CPU、内存资源，并占满SYN等待队列。相应的，我们修改内核参数即可有效缓解。主要参数如下： 　　net.ipv4.tcp_syncookies = 1 　　net.ipv4.tcp_max_syn_backlog = 8192 　net.ipv4.tcp_synack_retries = 2 　　分别为启用SYN Cookie、设置SYN最大队列长度以及设置SYN+ACK最大重试次数。

#### HTTP Flood防御 　　HTTP Flood攻击防御主要通过缓存的方式进行，尽量由设备的缓存直接返回结果来保护后端业务。大型的互联网企业，会有庞大的CDN节点缓存内容。

#### Slowloris攻击防御比较简单，主要方案有两个。 　　第一个是统计每个TCP连接的时长并计算单位时间内通过的报文数量即可做精确识别。一个TCP连接中，HTTP报文太少和报文太多都是不正常的，过少可能是慢速连接攻击，过多可能是使用HTTP 1.1协议进行的HTTP Flood攻击，在一个TCP连接中发送多个HTTP请求。 　　第二个是限制HTTP头部传输的最大许可时间。超过指定时间HTTP Header还没有传输完成，直接判定源IP地址为慢速连接攻击，中断连接并加入黑名单。

# 性能优化

## 要进行优化，先得找到性能瓶颈

### 使用Profiler时，重点需要关注： 1）花时间多的函数以优化其算法， 2）调用次数巨多的函数——如果一个函数每秒被调用300K次，你只需要优化出0.001毫秒，那也是相当大的优化。

### 如果你觉得需要程序有更好的执行速度，那么，最基本的方法就是使用一个profiler和愿意去查看一下其汇编代码以找到程序的瓶颈

### 只有找到了程序的瓶颈，此时才是真正在思考如何去改进的时候，比如思考一个更好的算法，使用更快的语言优化，等等。 常规的做法是制胜法宝是挑选一个最佳的算法而不是进行微优化。

### 没有Profiler，你不会知道问题在哪里，不去看汇编，你可能知道问题所在，但你往往不知道为什么

## 性能调优

### 性能定义

#### Throughput ，吞吐量。也就是每秒钟可以处理的请求数，任务数。 Latency， 系统延迟。也就是系统在处理一个请求或一个任务时的延迟。

#### Throughput越大，Latency会越差。因为请求量过大，系统太繁忙，所以响应速度自然会低。 Latency越好，能支持的Throughput就会越高。因为Latency短说明处理速度快，于是就可以处理更多的请求。

### 性能测试

#### 需要定义Latency这个值

#### 开发性能测试工具，一个工具用来制造高强度的Throughput，另一个工具用来测量Latency。

#### 开始性能测试。你需要不断地提升测试的Throughput，然后观察系统的负载情况，如果系统顶得住，那就观察Latency的值。这样，你就可以找到系统的最大负载，并且你可以知道系统的响应延时是多少。

#### 关于Latency，如果吞吐量很少，这个值估计会非常稳定，当吞吐量越来越大时，系统的Latency会出现非常剧烈的抖动，所以，我们在测量Latency的时候，我们需要注意到Latency的分布，也就是说，有百分之几的在我们允许的范围，有百分之几的超出了，有百分之几的完全不可接受。也许，平均下来的Latency达标了，但是其中仅有50%的达到了我们可接受的范围。那也没有意义。

#### 关于性能测试，我们还需要定义一个时间段。比如：在某个吞吐量上持续15分钟。因为当负载到达的时候，系统会变得不稳定，当过了一两分钟后，系统才会稳定。另外，也有可能是，你的系统在这个负载下前几分钟还表现正常，然后就不稳定了，甚至垮了。所以，需要这么一段时间。这个值，我们叫做峰值极限。

#### 性能测试还需要做Soak Test，也就是在某个吞吐量下，系统可以持续跑一周甚至更长。这个值，我们叫做系统的正常运行的负载极限。

## 性能瓶颈

### 查看操作系统负载

#### 首要需要看的是操作系统的报告。看看操作系统的CPU利用率，看看内存使用率，看看操作系统的IO，还有网络的IO，网络链接数，等等。Windows下的perfmon是一个很不错的工具，Linux下也有很多相关的命令和工具，比如：SystemTap，LatencyTOP，vmstat, sar, iostat, top, tcpdump等等 。通过观察这些数据，我们就可以知道我们的软件的性能基本上出在哪里

##### 1）先看CPU利用率，如果CPU利用率不高，但是系统的Throughput和Latency上不去了，这说明我们的程序并没有忙于计算，而是忙于别的一些事，比如IO。（另外，CPU的利用率还要看内核态的和用户态的，内核态的一上去了，整个系统的性能就下来了。而对于多核CPU来说，CPU 0 是相当关键的，如果CPU 0的负载高，那么会影响其它核的性能，因为CPU各核间是需要有调度的，这靠CPU0完成）

##### 2）然后，我们可以看一下IO大不大，IO和CPU一般是反着来的，CPU利用率高则IO不大，IO大则CPU就小。关于IO，我们要看三个事，一个是磁盘文件IO，一个是驱动程序的IO（如：网卡），一个是内存换页率。这三个事都会影响系统性能。

##### 3）然后，查看一下网络带宽使用情况，在Linux下，你可以使用iftop, iptraf, ntop, tcpdump这些命令来查看。或是用Wireshark来查看。

##### 4）如果CPU不高，IO不高，内存使用不高，网络带宽使用不高。但是系统的性能上不去。这说明你的程序有问题，比如，你的程序被阻塞了。可能是因为等那个锁，可能是因为等某个资源，或者是在切换上下文。

#### 使用Profiler测试

##### 使用性能检测工具，也就是使用某个Profiler来差看一下我们程序的运行性能。如：Java的JProfiler/TPTP/CodePro Profiler，GNU的gprof，IBM的PurifyPlus，Intel的VTune，AMD的CodeAnalyst，还有Linux下的OProfile/perf，后面两个可以让你对你的代码优化到CPU的微指令级别，如果你关心CPU的L1/L2的缓存调优，那么你需要考虑一下使用VTune。 使用这些Profiler工具，可以让你程序中各个模块函数甚至指令的很多东西，如：运行的时间 ，调用的次数，CPU的利用率，等等。这些东西对我们来说非常有用。

###### 因为Profiler会让你的程序运行的性能变低，像PurifyPlus这样的工具会在你的代码中插入很多代码，会导致你的程序运行效率变低，从而没发测试出在高吞吐量下的系统的性能，对此，一般有两个方法来定位系统瓶颈： 1）在你的代码中自己做统计，使用微秒级的计时器和函数调用计算器，每隔10秒把统计log到文件中。 2）分段注释你的代码块，让一些函数空转，做Hard Code的Mock，然后再测试一下系统的Throughput和Latency是否有质的变化，如果有，那么被注释的函数就是性能瓶颈，再在这个函数体内注释代码，直到找到最耗性能的语句。

## 常见的系统瓶颈

### 性能优化的几个策略

#### 用空间换时间。各种cache如CPU L1/L2/RAM到硬盘，都是用空间来换时间的策略。这样策略基本上是把计算的过程一步一步的保存或缓存下来，这样就不用每次用的时候都要再计算一遍，比如数据缓冲，CDN，等。这样的策略还表现为冗余数据，比如数据镜象，负载均衡什么的。

#### 用时间换空间。有时候，少量的空间可能性能会更好，比如网络传输，如果有一些压缩数据的算法（如前些天说的“Huffman 编码压缩算法” 和 “rsync 的核心算法”），这样的算法其实很耗时，但是因为瓶颈在网络传输，所以用时间来换空间反而能省时间。

#### 简化代码。最高效的程序就是不执行任何代码的程序，所以，代码越少性能就越高。关于代码级优化的技术大学里的教科书有很多示例了。如：减少循环的层数，减少递归，在循环中少声明变量，少做分配和释放内存的操作，尽量把循环体内的表达式抽到循环外，条件表达的中的多个条件判断的次序，尽量在程序启动时把一些东西准备好，注意函数调用的开销（栈上开销），注意面向对象语言中临时对象的开销，小心使用异常（不要用异常来检查一些可接受可忽略并经常发生的错误），…… 等等，等等，这连东西需要我们非常了解编程语言和常用的库。

#### 并行处理。如果CPU只有一个核，你要玩多进程，多线程，对于计算密集型的软件会反而更慢（因为操作系统调度和切换开销很大），CPU的核多了才能真正体现出多进程多线程的优势。并行处理需要我们的程序有Scalability，不能水平或垂直扩展的程序无法进行并行处理。从架构上来说，这表再为——是否可以做到不改代码只是加加机器就可以完成性能提升？

### 根据2：8原则来说，20%的代码耗了你80%的性能，找到那20%的代码，你就可以优化那80%的性能

### 算法调优。算法非常重要，好的算法会有更好的性能

#### 过滤算法

#### 哈希算法

#### 分而治之和预处理

### 代码调优

#### 字符串操作。这是最费系统性能的事了，无论是strcpy, strcat还是strlen，最需要注意的是字符串子串匹配。所以，能用整型最好用整型

#### 多线程调优。多线程瓶颈就在于互斥和同步的锁上，以及线程上下文切换的成本，怎么样的少用锁或不用锁是根本。乐观锁可以解决性能问题），此外，还有读写锁也可以解决大多数是读操作的并发的性能问题。 线程安全的智能指针也会上锁，消耗性能

#### 内存分配。不要小看程序的内存分配。malloc/realloc/calloc这样的系统调非常耗时，尤其是当内存出现碎片的时候。

##### 当然解决内存碎片的问题还是通过内存池，具体来说是一系列不同尺寸的内存池

##### 说到内存池就需要说一下池化技术。比如线程池，连接池等。池化技术对于一些短作业来说（如http服务） 相当相当的有效。这项技术可以减少链接建立，线程创建的开销，从而提高性能。

#### 异步操作。我们知道Unix下的文件操作是有block和non-block的方式的，像有些系统调用也是block式的，如：Socket下的select，Windows下的WaitforObject之类的，如果我们的程序是同步操作，那么会非常影响性能，我们可以改成异步的，但是改成异步的方式会让你的程序变复杂。异步方式一般要通过队列，要注间队列的性能问题，另外，异步下的状态通知通常是个问题，比如消息事件通知方式，有callback方式，等，这些方式同样可能会影响你的性能。但是通常来说，异步操作会让性能的吞吐率有很大提升（Throughput），但是会牺牲系统的响应时间（latency）。这需要业务上支持。

#### 语言和代码库。我们要熟悉语言以及所使用的函数库或类库的性能。比如：STL中的很多容器分配了内存后，那怕你删除元素，内存也不会回收，其会造成内存泄露的假像，并可能造成内存碎片问题。再如，STL某些容器的size()==0 和 empty()是不一样的，因为，size()是O(n)复杂度，empty()是O(1)的复杂度，这个要小心

### 网络调优

#### TCP调优

##### TCP链接是有很多开销的，一个是会占用文件描述符，另一个是会开缓存，一般来说一个系统可以支持的TCP链接数是有限的，我们需要清楚地认识到TCP链接对系统的开销是很大的。正是因为TCP是耗资源的，所以，很多攻击都是让你系统上出现大量的TCP链接，把你的系统资源耗尽。比如著名的SYNC Flood攻击。

##### 所以，我们要注意配置KeepAlive参数，这个参数的意思是定义一个时间，如果链接上没有数据传输，系统会在这个时间发一个包，如果没有收到回应，那么TCP就认为链接断了，然后就会把链接关闭，这样可以回收系统资源开销。（注：HTTP层上也有KeepAlive参数）对于像HTTP这样的短链接，设置一个1-2分钟的keepalive非常重要。这可以在一定程度上防止DoS攻击。 net.ipv4.tcp_keepalive_probes = 5 net.ipv4.tcp_keepalive_intvl = 20 net.ipv4.tcp_fin_timeout = 30

##### 对于TCP的TIME_WAIT这个状态，主动关闭的一方进入TIME_WAIT状态，TIME_WAIT状态将持续2个MSL(Max Segment Lifetime)，默认为4分钟，TIME_WAIT状态下的资源不能回收。有大量的TIME_WAIT链接的情况一般是在HTTP服务器上。对此，有两个参数需要注意， net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_tw_recycle=1 前者表示重用TIME_WAIT，后者表示回收TIME_WAIT的资源。

##### TCP还有一个重要的概念叫RWIN（TCP Receive Window Size），这个东西的意思是，我一个TCP链接在没有向Sender发出ack时可以接收到的最大的数据包。为什么这个很重要？因为如果Sender没有收到Receiver发过来ack，Sender就会停止发送数据并会等一段时间，如果超时，那么就会重传。这就是为什么TCP链接是可靠链接的原因。重传还不是最严重的，如果有丢包发生的话，TCP的带宽使用率会马上受到影响（会盲目减半），再丢包，再减半，然后如果不丢包了，就逐步恢复。 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216

##### 一般来说，理论上的RWIN应该设置成：吞吐量 * 回路时间。Sender端的buffer应该和RWIN有一样的大小，因为Sender端发送完数据后要等Receiver端确认，如果网络延时很大，buffer过小了，确认的次数就会多，于是性能就不高，对网络的利用率也就不高了。也就是说，对于延迟大的网络，我们需要大的buffer，这样可以少一点ack，多一些数据，对于响应快一点的网络，可以少一些buffer。因为，如果有丢包（没有收到ack），buffer过大可能会有问题，因为这会让TCP重传所有的数据，反而影响网络性能。（当然，网络差的情况下，就别玩什么高性能了） 所以，高性能的网络重要的是要让网络丢包率非常非常地小（基本上是用在LAN里），如果网络基本是可信的，这样用大一点的buffer会有更好的网络传输性能（来来回回太多太影响性能了）。

#### UDP调优

##### 重点说一样，那就是MTU——最大传输单元（其实这对TCP也一样，因为这是链路层上的东西）。所谓最大传输单元，你可以想像成是公路上的公交车，假设一个公交车可以最多坐70人，带宽就像是公路的车道数一样，如果一条路上最多可以容下100辆公交车，那意味着我最多可以运送7000人，但是如果公交车坐不满，比如平均每辆车只有20人，那么我只运送了2000人，于是我公路资源（带宽资源）就被浪费了。 所以，我们对于一个UDP的包，我们要尽量地让他大到MTU的最大尺寸再往网络上传，这样可以最大化带宽利用率。对于这个MTU，以太网是1500字节，光纤是4352字节，802.11无线网是7981。但是，当我们用TCP/UDP发包的时候，我们的有效负载Payload要低于这个值，因为IP协议会加上20个字节，UDP会加上8个字节（TCP加的更多），所以，一般来说，你的一个UDP包的最大应该是1500-8-20=1472，这是你的数据的大小。

##### UDP还有一个最大的好处是multi-cast多播，这个技术对于你需要在内网里通知多台结点时非常方便和高效。而且，多播这种技术对于机会的水平扩展（需要增加机器来侦听多播信息）也很有利。

#### 网卡调优

##### 对于网卡，我们也是可以调优的，这对于千兆以及网网卡非常必要，在Linux下，我们可以用ifconfig查看网上的统计信息，如果我们看到overrun上有数据，我们就可能需要调整一下txqueuelen的尺寸（一般默认为1000），我们可以调大一些，如：ifconfig eth0 txqueuelen 5000。Linux下还有一个命令叫：ethtool可以用于设置网卡的缓冲区大小。在Windows下，我们可以在网卡适配器中的高级选项卡中调整相关的参数（如：Receive Buffers, Transmit Buffer等，不同的网卡有不同的参数）。把Buffer调大对于需要大数据量的网络传输非常有效。

#### 其它网络性能

##### 关于多路复用技术，也就是用一个线程来管理所有的TCP链接，有三个系统调用要重点注意：一个是select，这个系统调用只支持上限1024个链接，第二个是poll，其可以突破1024的限制，但是select和poll本质上是使用的轮询机制，轮询机制在链接多的时候性能很差，因主是O(n)的算法，所以，epoll出现了，epoll是操作系统内核支持的，仅当在链接活跃时，操作系统才会callback，这是由操作系统通知触发的，但其只有Linux Kernel 2.6以后才支持（准确说是2.5.44中引入的），当然，如果所有的链接都是活跃的，过多的使用epoll_ctl可能会比轮询的方式还影响性能，不过影响的不大。

##### DNS Lookup的系统调用要小心，比如：gethostbyaddr/gethostbyname，这个函数可能会相当的费时，因为其要到网络上去找域名，因为DNS的递归查询，会导致严重超时，而又不能通过设置什么参数来设置time out，对此你可以通过配置hosts文件来加快速度，或是自己在内存中管理对应表，在程序启动时查好，而不要在运行时每次都查。另外，在多线程下面，gethostbyname会一个更严重的问题，就是如果有一个线程的gethostbyname发生阻塞，其它线程都会在gethostbyname处发生阻塞，这个比较变态，要小心。这种到网上找信息的东西很多，比如，如果你的Linux使用了NIS，或是NFS，某些用户或文件相关的系统调用就很慢，所以要小心。

#### 系统调优

##### I/O模型

###### 前面说到过select/poll/epoll这三个系统调用，我们都知道，Unix/Linux下把所有的设备都当成文件来进行I/O，所以，那三个操作更应该算是I/O相关的系统调用。说到 I/O模型，这对于我们的I/O性能相当重要，我们知道，Unix/Linux经典的I/O方式是：

####### 第一种，同步阻塞式I/O，这个不说了。 第二种，同步无阻塞方式。其通过fctnl设置 O_NONBLOCK 来完成。 第三种，对于select/poll/epoll这三个是I/O不阻塞，但是在事件上阻塞，算是：I/O异步，事件同步的调用。 第四种，AIO方式。这种I/O 模型是一种处理与 I/O 并行的模型。I/O请求会立即返回，说明请求已经成功发起了。在后台完成I/O操作时，向应用程序发起通知，通知有两种方式：一种是产生一个信号，另一种是执行一个基于线程的回调函数来完成这次 I/O 处理过程。 第四种因为没有任何的阻塞，无论是I/O上，还是事件通知上，所以，其可以让你充分地利用CPU，比起第二种同步无阻塞好处就是，第二种要你一遍一遍地去轮询。Nginx之所所以高效，是其使用了epoll和AIO的方式来进行I/O的。

###### Windows下的I/O模型

####### a）一个是WriteFile系统调用，这个系统调用可以是同步阻塞的，也可以是同步无阻塞的，关于看文件是不是以Overlapped打开的。关于同步无阻塞，需要设置其最后一个参数Overlapped，微软叫Overlapped I/O，你需要WaitForSingleObject才能知道有没有写完成。这个系统调用的性能可想而知。

####### b）另一个叫WriteFileEx的系统调用，其可以实现异步I/O，并可以让你传入一个callback函数，等I/O结束后回调之， 但是这个回调的过程Windows是把callback函数放到了APC（Asynchronous Procedure Calls）的队列中，然后，只用当应用程序当前线程成为可被通知状态（Alterable）时，才会被回调。只有当你的线程使用了这几个函数时WaitForSingleObjectEx, WaitForMultipleObjectsEx, MsgWaitForMultipleObjectsEx, SignalObjectAndWait 和 SleepEx，线程才会成为Alterable状态。可见，这个模型，还是有wait，所以性能也不高。

####### c）然后是IOCP – IO Completion Port，IOCP会把I/O的结果放在一个队列中，但是，侦听这个队列的不是主线程，而是专门来干这个事的一个或多个线程去干（老的平台要你自己创建线程，新的平台是你可以创建一个线程池）。IOCP是一个线程池模型。这个和Linux下的AIO模型比较相似，但是实现方式和使用方式完全不一样。

###### 真正提高I/O性能方式是把和外设的I/O的次数降到最低，最好没有，所以，对于读来说，内存cache通常可以从质上提升性能，因为内存比外设快太多了。对于写来说，cache住要写的数据，少写几次，但是cache带来的问题就是实时性的问题，也就是latency会变大，我们需要在写的次数上和相应上做权衡。

##### 多核CPU调优

###### CPU0是很关键的，如果0号CPU被用得过狠的话，别的CPU性能也会下降，因为CPU0是有调整功能的，所以，我们不能任由操作系统负载均衡，因为我们自己更了解自己的程序，所以，我们可以手动地为其分配CPU核，而不会过多地占用CPU0，或是让我们关键进程和一堆别的进程挤在一起。

####### 对于Windows来说，我们可以通过“任务管理器”中的“进程”而中右键菜单中的“设置相关性……”（Set Affinity…）来设置并限制这个进程能被运行在哪些核上。 对于Linux来说，可以使用taskset命令来设置（你可以通过安装schedutils来安装这个命令：apt-get install schedutils）

###### 多核CPU还有一个技术叫NUMA技术（Non-Uniform Memory Access）。传统的多核运算是使用SMP(Symmetric Multi-Processor )模式，多个处理器共享一个集中的存储器和I/O总线。于是就会出现一致存储器访问的问题，一致性通常意味着性能问题。NUMA模式下，处理器被划分成多个node， 每个node有自己的本地存储器空间。

####### Linux下，对NUMA调优的命令是：numactl numactl --cpubind=0 --membind=0,1 myprogram arg1 arg2 当然，上面这个命令并不好，因为内存跨越了两个node，这非常不好。最好的方式是只让程序访问和自己运行一样的node，如： $ numactl --membind 1 --cpunodebind 1 --localalloc myapplication



##### 文件系统调优

###### 关于文件系统，因为文件系统也是有cache的，所以，为了让文件系统有最大的性能。首要的事情就是分配足够大的内存，这个非常关键，在Linux下可以使用free命令来查看 free/used/buffers/cached，理想来说，buffers和cached应该有40%左右。然后是一个快速的硬盘控制器，SCSI会好很多。最快的是Intel SSD 固态硬盘，速度超快，但是写次数有限。

###### 对于Linux的Ext3/4来说，几乎在所有情况下都有所帮助的一个参数是关闭文件系统访问时间，在/etc/fstab下看看你的文件系统 有没有noatime参数（一般来说应该有），还有一个是dealloc，它可以让系统在最后时刻决定写入文件发生时使用哪个块，可优化这个写入程序。还要注间一下三种日志模式：data=journal、data=ordered和data=writeback。默认设置data=ordered提供性能和防护之间的最佳平衡。 当然，对于这些来说，ext4的默认设置基本上是最佳优化了。

###### Linux下的查看I/O的命令—— iotop，可以让你看到各进程的磁盘读写的负载情况。 其它还有一些关于NFS、XFS的调优

##### 数据库调优

###### 数据库引擎调优

####### 数据库的锁的方式。这个非常非常地重要。并发情况下，锁是非常非常影响性能的。各种隔离级别，行锁，表锁，页锁，读写锁，事务锁，以及各种写优先还是读优先机制。性能最高的是不要锁，所以，分库分表，冗余数据，减少一致性事务处理，可以有效地提高性能。NoSQL就是牺牲了一致性和事务处理，并冗余数据，从而达到了分布式和高性能

####### 数据库的存储机制。不但要搞清楚各种类型字段是怎么存储的，更重要的是数据库的数据存储方式，是怎么分区的，是怎么管理的，比如Oracle的数据文件，表空间，段，等等。了解清楚这个机制可以减轻很多的I/O负载。比如：MySQL下使用show engines;可以看到各种存储引擎的支持。不同的存储引擎有不同的侧重点，针对不同的业务或数据库设计会让你有不同的性能。

####### 数据库的分布式策略。最简单的就是复制或镜像，需要了解分布式的一致性算法，或是主主同步，主从同步。通过了解这种技术的机理可以做到数据库级别的水平扩展。

###### SQL语句优化

####### 关于SQL语句的优化，首先也是要使用工具，比如：MySQL SQL Query Analyzer，Oracle SQL Performance Analyzer，或是微软SQL Query Analyzer，基本上来说，所有的RMDB都会有这样的工具，来让你查看你的应用中的SQL的性能问题。 还可以使用explain来看看SQL语句最终Execution Plan会是什么样的。 还有一点很重要，数据库的各种操作需要大量的内存，所以服务器的内存要够，优其应对那些多表查询的SQL语句，那是相当的耗内存。

####### 全表检索。比如：select * from user where lastname = “xxxx”，这样的SQL语句基本上是全表查找，线性复杂度O(n)，记录数越多，性能也越差（如：100条记录的查找要50ms，一百万条记录需要5分钟）。对于这种情况，我们可以有两种方法提高性能：一种方法是分表，把记录数降下来，另一种方法是建索引（为lastname建索引）。索引就像是key-value的数据结构一样，key就是where后面的字段，value就是物理行号，对索引的搜索复杂度是基本上是O(log(n)) ——用B-Tree实现索引（如：100条记录的查找要50ms，一百万条记录需要100ms）。

####### 索引。对于索引字段，最好不要在字段上做计算、类型转换、函数、空值判断、字段连接操作，这些操作都会破坏索引原本的性能。当然，索引一般都出现在Where或是Order by字句中，所以对Where和Order by子句中的子段最好不要进行计算操作，或是加上什么NOT之类的，或是使用什么函数。

####### 多表查询。关系型数据库最多的操作就是多表查询，多表查询主要有三个关键字，EXISTS，IN和JOIN（关于各种join，可以参看图解SQL的Join一文）。基本来说，现代的数据引擎对SQL语句优化得都挺好的，JOIN和IN/EXISTS在结果上有些不同，但性能基本上都差不多。有人说，EXISTS的性能要好于IN，IN的性能要好于JOIN，我各人觉得，这个还要看你的数据、schema和SQL语句的复杂度，对于一般的简单的情况来说，都差不多，所以千万不要使用过多的嵌套，千万不要让你的SQL太复杂，宁可使用几个简单的SQL也不要使用一个巨大无比的嵌套N级的SQL。还有人说，如果两个表的数据量差不多，Exists的性能可能会高于In，In可能会高于Join，如果这两个表一大一小，那么子查询中，Exists用大表，In则用小表。这个，我没有验证过，放在这里让大家讨论吧。另，有一篇关于SQL Server的文章大家可以看看《IN vs JOIN vs EXISTS》

####### JOIN操作。有人说，Join表的顺序会影响性能，只要Join的结果集是一样，性能和join的次序无关。因为后台的数据库引擎会帮我们优化的。Join有三种实现算法，嵌套循环，排序归并，和Hash式的Join。（MySQL只支持第一种）

####### 嵌套循环，就好像是我们常见的多重嵌套循环。注意，前面的索引说过，数据库的索引查找算法用的是B-Tree，这是O(log(n))的算法，所以，整个算法复法度应该是O(log(n)) * O(log(m)) 这样的。 Hash式的Join，主要解决嵌套循环的O(log(n))的复杂，使用一个临时的hash表来标记。 排序归并，意思是两个表按照查询字段排好序，然后再合并。当然，索引字段一般是排好序的。 还是那句话，具体要看什么样的数据，什么样的SQL语句，你才知道用哪种方法是最好的。

####### 部分结果集。我们知道MySQL里的Limit关键字，Oracle里的rownum，SQL Server里的Top都是在限制前几条的返回结果。这给了我们数据库引擎很多可以调优的空间。一般来说，返回top n的记录数据需要我们使用order by，注意在这里我们需要为order by的字段建立索引。有了被建索引的order by后，会让我们的select语句的性能不会被记录数的所影响。使用这个技术，一般来说我们前台会以分页方式来显现数据，Mysql用的是OFFSET，SQL Server用的是FETCH NEXT，这种Fetch的方式其实并不好是线性复杂度，所以，如果我们能够知道order by字段的第二页的起始值，我们就可以在where语句里直接使用>=的表达式来select，这种技术叫seek，而不是fetch，seek的性能比fetch要高很多。 字符串。正如我前面所说的，字符串操作对性能上有非常大的恶梦，所以，能用数据的情况就用数字，比如：时间，工号，等。

####### 全文检索。千万不要用Like之类的东西来做全文检索，如果要玩全文检索，可以尝试使用Sphinx。 其它。 不要select *，而是明确指出各个字段，如果有多个表，一定要在字段名前加上表名，不要让引擎去算。 不要用Having，因为其要遍历所有的记录。性能差得不能再差。 尽可能地使用UNION ALL 取代 UNION。 索引过多，insert和delete就会越慢。而update如果update多数索引，也会慢，但是如果只update一个，则只会影响一个索引表。 等等。

# redis

## 数据结构

### string/list/set/hash_set

## 持久化

### RDB快照

### AOF日志

## 缓存

## 集群

## 事务

## 队列/订阅

## 可以用作：数据库、缓存和消息中间件

## 优势

### ✎ 缓存管理：可以在必要时将无效的旧数据从内存中删除，为新数据腾出新的空间（过期的key的三种清理策略）。 ✎ 提供更大的灵活性：redis支持多种类型，并且采用key-value 的形式存储，key和value的大小限制都是512Mb,与编码无关，所以数据安全。但是memcached限制key最大为250字节，value为1MB，况且只支持String类型。Redis通过Hash来存储一个对象的字段和值，并且可以通过单个key来管理它们（设置值，设置过期时间），而且我们可以很方便的通过Linux命令或者redis-cli完成对key的管理。 ✎ redis提供主从复制：实现高可用的cache系统，支持集群中多个服务器之间的数据同步。 ✎ 数据持久化：redis可以通过两种方式将数据进行持久化，一定程度上规避缓存中的数据不稳定的问题，也可以在重启服务器时最快的恢复缓存中所需的数据，提高了效率的同时减轻了主数据库系统的开销。

## Redis为什么会快

### 完全基于内存：Redis是纯内存数据库，相对于读写磁盘，读写内存的速度就不是几倍几十倍了，一般，hash查找可以达到每秒百万次的数量级。 多路复用IO：“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗） 单线程的原子操作，避免上下文切换的时间和性能消耗；加上对内存中数据的处理速度，很自然的提高redis的吞吐量。 [内存+IO复用+单线程(无锁+无切换)]

## 缓存更新

### 在这个架构里面，缓存更新流程如下： (1) 业务完成DB更新后即返回请求 (2) 数据订阅通过日志解析方式实时解析并订阅DB的增量更新数据，当发现DB有数据更新时，将增量数据推送给下游消费者 (3) 下游消费业务一旦接收到增量更新数据，即调用消费线程进行缓存更新 从上面的缓存失效流程，可以看出这种缓存失效机制： (1) 更新路径短，延迟低： 缓存失效为异步流程，业务更新DB完成后直接返回，不需要关心缓存失效流程，整个更新路径短，更新延迟低 (2) 应用简单可靠：应用无需实现复杂双写逻辑，只需启动异步线程监听增量数据，更新缓存数据即可 (3) 应用更新无性能消耗：因为数据订阅是通过解析DB的增量日志来获取增量数据，获取数据的过程对业务、DB性能无损



# NGINX

## NGINX 是一个事件处理器，一个从内核接收所有发生在连接上的事件信息的控制器，然后给操作系统发布命令。实际上，NGINX 通过编排操作系统做了全部的辛苦工作，操作系统则做了读字节和发送字节等日常工作。

## 对 NGINX 而言，线程池就是充当配送服务的角色，它由一个任务队列和一组处理队列的线程组成。一旦工作进程需要处理某个可能的长操作，不用自己操作，将其作为一个任务放出线程池的队列，接着会被某个空闲线程提取处理。

## 工作进程把阻塞操作转给线程池: 硬盘读取操作通常就是阻塞操作，不过NGINX中的线程池可以用来处理任何在主工作周期不适合处理的任务



# P2P

## 对等式网络（peer-to-peer， 简称P2P），又称点对点技术，是无中心服务器、依靠用户群（peers）交换信息的互联网体系，它的作用在于，减低以往网路传输中的节点，以降低资料遗失的风险。与有中心服务器的中央网络系统不同，对等网络的每个用户端既是一个节点，也有服务器的功能，任何一个节点无法直接找到其他节点，必须依靠其户群进行信息交流。 P2P节点能遍布整个互联网，也给包括开发者在内的任何人、组织、或政府带来监控难题。P2P在网络隐私要求高和文件共享领域中，得到了广泛的应用。使用纯P2P技术的网络系统有比特币、Gnutella，或自由网等。另外，P2P技术也被使用在类似VoIP等实时媒体业务的数据通信中。有些网络（如Napster、OpenNAP，或IRC @find）包括搜索的一些功能，也使用客户端-服务器结构，而使用P2P结构来实现另外一些功能。这种网络设计模型不同于客户端-服务器模型，在客户端-服务器模型中通信通常来往于一个中央服务器。

# BT

## BitTorrent协议（简称BT，俗称比特洪流、BT下载）是用在对等网络中文件分享的网络协议程序。和点对点（point-to-point）的协议程序不同，它是用户群对用户群（peer-to-peer），而且用户越多，下载同一文件的人越多，下载该档案的速度越快。且下载后，继续维持上传的状态，就可以“分享”，成为其用户端节点下载的种子文件（.torrent），同时上传及下载

## 原理简述 BT原理在该图示中，由不同颜色区分的是某一文件的不同部分，当传输开始时，只有种子发布者拥有全部文件，在传输进行中，有部分用户获得部分文件（带颜色线条表示），随着传输的继续，文件已经全部公布在系统中，此时，种子拥有者可以退出，也不会影响该文件的传播 普通的HTTP／FTP下载使用TCP/IP协议，BitTorrent协议是架构于TCP/IP协议之上的一个P2P文件传输通信协议，处于TCP/IP结构的应用层。BitTorrent协议本身也包含了很多具体的内容协议和扩展协议，并在不断扩充中。 根据BitTorrent协议，文件发布者会根据要发布的文件生成提供一个.torrent文件，即种子文件，也简称为“种子”。 种子文件本质上是文本文件，包含Tracker信息和文件信息两部分。Tracker信息主要是BT下载中需要用到的Tracker服务器的地址和针对Tracker服务器的设置，文件信息是根据对目标文件的计算生成的，计算结果根据BitTorrent协议内的Bencode规则进行编码。它的主要原理是需要把提供下载的文件虚拟分成大小相等的块，块大小必须为2k的整数次方（由于是虚拟分块，硬盘上并不产生各个块文件），并把每个块的索引信息和Hash验证码写入种子文件中；所以，种子文件就是被下载文件的“索引”。 下载者要下载文件内容，需要先得到相应的种子文件，然后使用BT客户端软件进行下载。 下载时，BT客户端首先解析种子文件得到Tracker地址，然后连接Tracker服务器。Tracker服务器回应下载者的请求，提供下载者其他下载者（包括发布者）的IP。下载者再连接其他下载者，根据种子文件，两者分别告知对方自己已经有的块，然后交换对方所没有的数据。此时不需要其他服务器参与，分散了单个线路上的数据流量，因此减轻了服务器负担。 下载者每得到一个块，需要算出下载块的Hash验证码与种子文件中的对比，如果一样则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容准确性的问题。 一般的HTTP/FTP下载，发布文件仅在某个或某几个服务器，下载的人太多，服务器的带宽很易不胜负荷，变得很慢。而BitTorrent协议下载的特点是，下载的人越多，提供的带宽也越多，下载速度就越快。同时，拥有完整文件的用户也会越来越多，使文件的“寿命”不断延长。 为了解决某些用户“下完就跑”的现象，在非官方BitTorrent协议中还存在一种慢慢开放下载内容的超级种子的算法。

# DHT网络

## 目前，又发展出DHT网络技术，可以在无Tracker的情况下下载。 DHT全称为分布式哈希表（Distributed Hash Table），是一种分布式存储方法。在不需要服务器的情况下，每个客户端负责一个小范围的路由，并负责存储一小部分数据，从而实现整个DHT网络的寻址和存储。使用支持该技术的BT下载软件，用户无需连上Tracker就可以下载，因为软件会在DHT网络中寻找下载同一文件的其他用户并与之通讯，开始下载任务。

# Bitcoin

## 特币（英语：Bitcoin[注 2]，缩写：BTC）被部分观点认为是一种去中心化，非普遍全球可支付的电子加密货币[6]，而多数国家则认为比特币属于虚拟商品，并非货币[7]。比特币由中本聪（又译中本哲史）[注 3]（化名）于2009年1月3日，基于无国界的对等网络，用共识主动性开源软件发明创立。自比特币出现一直至今，比特币一直是目前法币市场总值最高的加密货币[8]。 任何人皆可参与比特币活动，可以通过称为挖矿的电脑运算来发行。比特币协议数量上限为2100万个，以避免通货膨胀问题。使用比特币是通过私钥作为数字签名，允许个人直接支付给他人，不需经过如银行、清算中心、证券商等第三方机构，从而避免了高手续费、繁琐流程以及受监管性的问题[9]，任何用户只要拥有可连接互联网的数字设备皆可使用。2017年8月1日出现比特币现金(Bitcoin Cash, BCH)，是一个比特币的硬分叉。

# blockchain

## 区块链（英语：blockchain 或 block chain）[1]是用分布式数据库识别、传播和记载信息的智能化对等网络，也称为价值互联网。[2][3] 中本聪在2008年，于《比特币白皮书》[4]中提出“区块链”概念，并在2009年创立了比特币社会网络，开发出第一个区块，即“创世区块”。[5] 区块链共享价值体系首先被众多的加密货币效仿，并在工作量证明上和算法上进行了改进，如采用权益证明和SCrypt算法。随后，区块链生态系统在全球不断进化，出现了首次代币发售ICO；智能合约区块链以太坊；“轻所有权、重使用权”的资产代币化共享经济；[6] 和区块链国家。目前，人们正在利用这一共享价值体系，在各行各业开发去中心化电脑程序(Decentralized applications, Dapp)[7]，在全球各地构建去中心化自主组织和去中心化自主社区(Decentralized autonomous society, DAS)[8]。

## 通过区块链这个简单设计，实现了中本聪最初关于一个无政府主义货币的全部愿望和设计理念 1、完美模拟黄金 1）总量有控制（可避免通涨） 2）开采有代价 3）可无限分割 2、基本实现了中本聪基于无政府主义的全部货币理念 1）去中心、平等的参与权 2）永续记录、内容透明 3）保护用户隐私 4）降低交易成本

## 二、用一个几乎最简单的规则漂亮解决了『拜占庭将军问题』 1）保持沉默无所谓 2）不作为者无影响 1）撒谎无效 2）攻击不能

# 云计算

## 服务模式

### SaaS(软件即服务)

### PaaS(平台即服务)

### IaaS(基础设施即服务)

### SECaaS(安全即服务)

## 应用

### 云教育

### 云物联

### 云社交

### 云安全

### 云存储

### 云政务

## AWS ( Amazon EC2 · Amazon S3 ) · Google云平台 · OpenStack · Rackspace Cloud

## 基础设施

### 云数据库 · 数据中心 · 分布式文件系统 · 硬件虚拟化 · Web服务 · 虚拟设备 · 多租户技术 · 虚拟私有云 · 微服务

## 技术

### 云数据库 · 数据中心 · 分布式文件系统 · 硬件虚拟化 · Web服务 · 虚拟设备 · 多租户技术 · 虚拟私有云 · 微服务

## 互联网上的云计算服务特征和自然界的云、水循环具有一定的相似性，因此，云是一个相当贴切的比喻。根据美国国家标准和技术研究院的定义，云计算服务应该具备以下几条特征：[10] 随需应变自助服务。 随时随地用任何网络设备访问。 多人共享资源池。 快速重新部署灵活度。 可被监控与量测的服务。 一般认为还有如下特征： 基于虚拟化技术快速部署资源或获得服务。 减少用户终端的处理负担。 降低了用户对于IT专业知识的依赖。
